{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b18015ed",
   "metadata": {},
   "source": [
    "# Generalized Riesz Regression (GRR) — End-to-end examples\n",
    "\n",
    "This notebook demonstrates how to use the `genriesz` package to estimate several common functionals:\n",
    "\n",
    "- **ATE** (average treatment effect)\n",
    "- **AME** (average marginal effect / average derivative)\n",
    "- **Average policy effect**\n",
    "\n",
    "## Installation\n",
    "\n",
    "If you are running this notebook from the repository, an editable install is convenient:\n",
    "\n",
    "```bash\n",
    "pip install -e .[sklearn,torch]\n",
    "```\n",
    "\n",
    "(You can omit `sklearn` or `torch` if you are not running those sections.)\n",
    "\n",
    "\n",
    "It also shows how to plug in different **bases**:\n",
    "\n",
    "- Polynomial basis\n",
    "- RKHS-style RBF features (random Fourier features)\n",
    "- Random-forest leaf indicator features (scikit-learn)\n",
    "- Neural-network embedding features (PyTorch)\n",
    "\n",
    "Finally, it shows how to report **DM / IPW / AIPW** simultaneously with **cross-fitting**, **confidence intervals**, and **p-values**.\n",
    "\n",
    "> Notes  \n",
    "> - The **link function is automatic**: it is induced by the chosen Bregman generator (e.g. `UKLGenerator`).  \n",
    "> - To keep **exact ACB structure** for the chosen basis, the GRR solver stays **linear in parameters**.  \n",
    ">   Random forests / neural nets are used as **fixed feature maps** (bases), not trained end-to-end inside the GRR objective.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3bdaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Optional: enable local \"src/\" imports when running from the repository ---\n",
    "root = Path.cwd().resolve()\n",
    "for _ in range(6):\n",
    "    if (root / \"src\" / \"genriesz\").exists():\n",
    "        sys.path.insert(0, str(root / \"src\"))\n",
    "        break\n",
    "    root = root.parent\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from genriesz import (\n",
    "    grr_functional,\n",
    "    grr_ate,\n",
    "    grr_ame,\n",
    "    grr_policy_effect,\n",
    "    ATEFunctional,\n",
    "    AverageDerivativeFunctional,\n",
    "    PolicyEffectFunctional,\n",
    "    PolynomialBasis,\n",
    "    RBFRandomFourierBasis,\n",
    "    TreatmentInteractionBasis,\n",
    "    SquaredGenerator,\n",
    "    UKLGenerator,\n",
    ")\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49be2d11",
   "metadata": {},
   "source": [
    "## Synthetic data generators\n",
    "\n",
    "We will use small synthetic datasets so the notebook runs quickly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4a821b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ate_data(n=800, d_z=5, seed=0):\n",
    "    \"\"\"Binary-treatment DGP with constant ATE tau.\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    Z = rng.normal(size=(n, d_z))\n",
    "\n",
    "    logits = 0.6 * Z[:, 0] - 0.4 * Z[:, 1] + 0.2 * Z[:, 2]\n",
    "    e = 1.0 / (1.0 + np.exp(-logits))\n",
    "    D = rng.binomial(1, e, size=n).astype(float)\n",
    "\n",
    "    tau = 1.0\n",
    "    mu0 = np.sin(Z[:, 0]) + 0.25 * Z[:, 1] ** 2\n",
    "    Y = mu0 + tau * D + rng.normal(scale=1.0, size=n)\n",
    "\n",
    "    X = np.column_stack([D, Z])  # X = [D, Z]\n",
    "    return X, Y, tau, Z, D\n",
    "\n",
    "\n",
    "def make_ame_data(n=1200, d_z=3, seed=0):\n",
    "    \"\"\"Continuous-treatment DGP with known AME.\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    Z = rng.normal(size=(n, d_z))\n",
    "\n",
    "    D = 0.7 * Z[:, 0] - 0.3 * Z[:, 1] + rng.normal(scale=1.0, size=n)\n",
    "    mu = np.sin(Z[:, 0]) + 0.25 * Z[:, 1] ** 2 + (1.0 + 0.5 * Z[:, 1]) * D\n",
    "    Y = mu + rng.normal(scale=1.0, size=n)\n",
    "\n",
    "    # True AME = E[1 + 0.5 Z1] = 1 (because E[Z1]=0)\n",
    "    true_ame = 1.0\n",
    "\n",
    "    X = np.column_stack([D, Z])  # X = [D, Z]\n",
    "    return X, Y, true_ame\n",
    "\n",
    "\n",
    "def make_policy_data(n=1200, d_z=3, seed=0):\n",
    "    \"\"\"Binary-treatment DGP with heterogeneous effects for a policy-effect demo.\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    Z = rng.normal(size=(n, d_z))\n",
    "\n",
    "    logits = 0.6 * Z[:, 0] - 0.4 * Z[:, 1]\n",
    "    e = 1.0 / (1.0 + np.exp(-logits))\n",
    "    D = rng.binomial(1, e, size=n).astype(float)\n",
    "\n",
    "    tau = 1.0 + 0.5 * Z[:, 0]  # heterogeneous\n",
    "    mu0 = Z[:, 0] + 0.25 * Z[:, 1] ** 2\n",
    "    Y = mu0 + tau * D + rng.normal(scale=1.0, size=n)\n",
    "\n",
    "    X = np.column_stack([D, Z])\n",
    "    return X, Y, Z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f7cc01",
   "metadata": {},
   "source": [
    "## Example 1 — ATE with a polynomial basis (and DM/IPW/AIPW)\n",
    "\n",
    "Here we use:\n",
    "\n",
    "- `m = ATEFunctional(...)`\n",
    "- a polynomial base basis on `Z`\n",
    "- `TreatmentInteractionBasis` to build `φ(D,Z) = [1, D, ψ(Z), D·ψ(Z)]`\n",
    "- `UKLGenerator` (automatic ACB link)\n",
    "- `grr_functional` to compute **DM / IPW / AIPW** with cross-fitting\n",
    "\n",
    "We also demonstrate `outcome_models=\"both\"` so that the output includes:\n",
    "\n",
    "- **shared** outcome model: the default penalized linear model using the same basis as GRR\n",
    "- **separate** outcome model: a user-supplied model (here: a random forest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0d5911",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "X, Y, tau, Z, D = make_ate_data(n=800, d_z=5, seed=0)\n",
    "\n",
    "m = ATEFunctional(treatment_index=0)\n",
    "\n",
    "psi = PolynomialBasis(degree=2, include_bias=False)\n",
    "phi = TreatmentInteractionBasis(base_basis=psi)\n",
    "\n",
    "gen = UKLGenerator(C=1.0, branch_fn=lambda x: int(x[0] == 1.0)).as_generator()\n",
    "\n",
    "rf_outcome = RandomForestRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=6,\n",
    "    min_samples_leaf=10,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "res_poly = grr_functional(\n",
    "    X=X,\n",
    "    Y=Y,\n",
    "    m=m,\n",
    "    basis=phi,\n",
    "    generator=gen,\n",
    "    cross_fit=True,\n",
    "    folds=3,\n",
    "    estimators=(\"dm\", \"ipw\", \"aipw\"),\n",
    "    outcome_models=\"both\",          # fit shared + separate outcome models\n",
    "    outcome_model=rf_outcome,         # used for the separate DM/AIPW\n",
    "    riesz_penalty=\"l2\",\n",
    "    riesz_lam=1e-3,\n",
    "    max_iter=300,\n",
    "    tol=1e-9,\n",
    ")\n",
    "\n",
    "print(\"True ATE (by construction):\", tau)\n",
    "print(res_poly.summary_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91015ca",
   "metadata": {},
   "source": [
    "## Example 2 — ATE with an RKHS-style RBF basis (random Fourier features)\n",
    "\n",
    "This approximates an RBF kernel feature map using random Fourier features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21715946",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, tau, Z, D = make_ate_data(n=800, d_z=5, seed=1)\n",
    "\n",
    "psi_rff = RBFRandomFourierBasis(\n",
    "    n_features=150,\n",
    "    sigma=1.0,\n",
    "    standardize=True,\n",
    "    random_state=0,\n",
    ")\n",
    "phi_rff = TreatmentInteractionBasis(base_basis=psi_rff)\n",
    "\n",
    "gen = UKLGenerator(C=1.0, branch_fn=lambda x: int(x[0] == 1.0)).as_generator()\n",
    "\n",
    "res_rff = grr_ate(\n",
    "    X=X,\n",
    "    Y=Y,\n",
    "    basis=phi_rff,\n",
    "    generator=gen,\n",
    "    cross_fit=True,\n",
    "    folds=3,\n",
    "    estimators=(\"dm\", \"ipw\", \"aipw\"),\n",
    "    outcome_models=\"shared\",\n",
    "    riesz_penalty=\"l2\",\n",
    "    riesz_lam=1e-3,\n",
    "    max_iter=250,\n",
    "    tol=1e-9,\n",
    ")\n",
    "\n",
    "print(\"True ATE (by construction):\", tau)\n",
    "print(res_rff.summary_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad9572c",
   "metadata": {},
   "source": [
    "## Example 3 — Random forest leaf basis for the Riesz model (scikit-learn)\n",
    "\n",
    "You can use a tree ensemble as a **feature map** by encoding **leaf indices** as one-hot features.\n",
    "\n",
    "A simple pattern is:\n",
    "\n",
    "1. Fit a random forest **propensity model**: predict `D` from `Z`.\n",
    "2. Convert leaf indices into one-hot features `ψ(Z)`.\n",
    "3. Build `φ(D,Z)` with treatment interactions.\n",
    "4. Run GRR with the same (automatic) generator.\n",
    "\n",
    "This keeps GRR convex in `β` while giving a flexible, data-adaptive basis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9321cd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from genriesz.sklearn_basis import RandomForestLeafBasis\n",
    "\n",
    "X, Y, tau, Z, D = make_ate_data(n=800, d_z=5, seed=2)\n",
    "\n",
    "# Fit a modest forest to avoid an excessively large one-hot basis.\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=5,\n",
    "    min_samples_leaf=20,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "leaf_basis = RandomForestLeafBasis(rf, include_bias=False)\n",
    "leaf_basis.fit(Z, D)\n",
    "\n",
    "phi_leaf = TreatmentInteractionBasis(base_basis=leaf_basis)\n",
    "\n",
    "gen = UKLGenerator(C=1.0, branch_fn=lambda x: int(x[0] == 1.0)).as_generator()\n",
    "\n",
    "res_leaf = grr_ate(\n",
    "    X=X,\n",
    "    Y=Y,\n",
    "    basis=phi_leaf,\n",
    "    generator=gen,\n",
    "    cross_fit=True,\n",
    "    folds=3,\n",
    "    estimators=(\"dm\", \"ipw\", \"aipw\"),\n",
    "    outcome_models=\"shared\",\n",
    "    riesz_penalty=\"l2\",\n",
    "    riesz_lam=1e-3,\n",
    "    max_iter=250,\n",
    "    tol=1e-9,\n",
    ")\n",
    "\n",
    "print(\"True ATE (by construction):\", tau)\n",
    "print(res_leaf.summary_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730b1796",
   "metadata": {},
   "source": [
    "## Example 4 — Neural network embedding basis (PyTorch)\n",
    "\n",
    "A recommended way to use neural networks **without breaking the GLM-style GRR structure** is:\n",
    "\n",
    "1. Train an embedding network `ψ(Z)` on an auxiliary task (e.g., predict `D` from `Z`).\n",
    "2. Freeze the embedding network.\n",
    "3. Use its output as a fixed basis inside GRR.\n",
    "\n",
    "Below we train a small MLP embedding on the treatment prediction task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d9dd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from genriesz.torch_basis import MLPEmbeddingNet, TorchEmbeddingBasis\n",
    "\n",
    "X, Y, tau, Z, D = make_ate_data(n=800, d_z=5, seed=3)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# --- 1) Train an embedding network to predict D from Z ---\n",
    "embed = MLPEmbeddingNet(input_dim=Z.shape[1], hidden_dim=64, output_dim=16)\n",
    "head = nn.Linear(16, 1)\n",
    "model = nn.Sequential(embed, head)\n",
    "\n",
    "xt = torch.tensor(Z, dtype=torch.float32)\n",
    "yt = torch.tensor(D.reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "loader = DataLoader(TensorDataset(xt, yt), batch_size=256, shuffle=True)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "opt = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "model.train()\n",
    "for _epoch in range(4):\n",
    "    for xb, yb in loader:\n",
    "        opt.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "# Freeze the embedding\n",
    "embed.eval()\n",
    "for p in embed.parameters():\n",
    "    p.requires_grad_(False)\n",
    "\n",
    "# --- 2) Wrap as a NumPy-returning basis ψ(Z) ---\n",
    "psi_nn = TorchEmbeddingBasis(embed, device=\"cpu\", include_bias=False)\n",
    "\n",
    "# --- 3) Build φ(D,Z) and run GRR ---\n",
    "phi_nn = TreatmentInteractionBasis(base_basis=psi_nn)\n",
    "gen = UKLGenerator(C=1.0, branch_fn=lambda x: int(x[0] == 1.0)).as_generator()\n",
    "\n",
    "res_nn = grr_ate(\n",
    "    X=X,\n",
    "    Y=Y,\n",
    "    basis=phi_nn,\n",
    "    generator=gen,\n",
    "    cross_fit=True,\n",
    "    folds=3,\n",
    "    estimators=(\"dm\", \"ipw\", \"aipw\"),\n",
    "    outcome_models=\"shared\",\n",
    "    riesz_penalty=\"l2\",\n",
    "    riesz_lam=1e-3,\n",
    "    max_iter=250,\n",
    "    tol=1e-9,\n",
    ")\n",
    "\n",
    "print(\"True ATE (by construction):\", tau)\n",
    "print(res_nn.summary_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7893b019",
   "metadata": {},
   "source": [
    "## Example 5 — AME (average marginal effect / average derivative)\n",
    "\n",
    "We estimate the average derivative of the regression function with respect to the treatment coordinate.\n",
    "\n",
    "Here we use `SquaredGenerator`, which induces a linear link.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258f3c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, true_ame = make_ame_data(n=1200, d_z=3, seed=0)\n",
    "\n",
    "phi = PolynomialBasis(degree=2, include_bias=True)\n",
    "gen = SquaredGenerator(C=0.0).as_generator()\n",
    "\n",
    "res_ame = grr_ame(\n",
    "    X=X,\n",
    "    Y=Y,\n",
    "    coordinate=0,\n",
    "    eps=1e-4,\n",
    "    basis=phi,\n",
    "    generator=gen,\n",
    "    cross_fit=True,\n",
    "    folds=3,\n",
    "    estimators=(\"dm\", \"ipw\", \"aipw\"),\n",
    "    outcome_models=\"shared\",\n",
    "    riesz_penalty=\"l2\",\n",
    "    riesz_lam=1e-3,\n",
    "    max_iter=250,\n",
    "    tol=1e-9,\n",
    ")\n",
    "\n",
    "print(\"True AME (by construction):\", true_ame)\n",
    "print(res_ame.summary_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d446eb8a",
   "metadata": {},
   "source": [
    "## Example 6 — Average policy effect\n",
    "\n",
    "We compare two policies:\n",
    "\n",
    "- `π1(z)`: treat if `z0 > 0`\n",
    "- `π0(z)`: never treat\n",
    "\n",
    "This reduces to the ATE when `π1(z)=1` and `π0(z)=0`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8a5174",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, Z = make_policy_data(n=1200, d_z=3, seed=1)\n",
    "\n",
    "def pi1(z: np.ndarray) -> float:\n",
    "    return float(z[0] > 0.0)\n",
    "\n",
    "def pi0(_z: np.ndarray) -> float:\n",
    "    return 0.0\n",
    "\n",
    "# ATE-style basis on Z + treatment interactions.\n",
    "psi = PolynomialBasis(degree=2, include_bias=False)\n",
    "phi = TreatmentInteractionBasis(base_basis=psi)\n",
    "\n",
    "gen = UKLGenerator(C=1.0, branch_fn=lambda x: int(x[0] == 1.0)).as_generator()\n",
    "\n",
    "res_policy = grr_policy_effect(\n",
    "    X=X,\n",
    "    Y=Y,\n",
    "    policy_1=pi1,\n",
    "    policy_0=pi0,\n",
    "    treatment_index=0,\n",
    "    basis=phi,\n",
    "    generator=gen,\n",
    "    cross_fit=True,\n",
    "    folds=3,\n",
    "    estimators=(\"dm\", \"ipw\", \"aipw\"),\n",
    "    outcome_models=\"shared\",\n",
    "    riesz_penalty=\"l2\",\n",
    "    riesz_lam=1e-3,\n",
    "    max_iter=250,\n",
    "    tol=1e-9,\n",
    ")\n",
    "\n",
    "print(res_policy.summary_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 7 — Nearest-neighbor matching as a Riesz/LSIF special case\n",
    "\n",
    "The paper shows that classical NN matching weights can be interpreted through a\n",
    "squared-loss Riesz / LSIF construction using a *catchment-area* indicator basis.\n",
    "\n",
    "Here we compute the matching-style IPW estimate\n",
    "\n",
    "$$\n",
    "\\hat\\tau = \\frac{1}{n}\\sum_i (2D_i-1)\\,\\hat w_i\\,Y_i,\n",
    "\\qquad \\hat w_i = 1 + \\frac{K_M(i)}{M},\n",
    "$$\n",
    "\n",
    "where $K_M(i)$ is the matched-times count (how often unit $i$ is selected among the\n",
    "$M$ nearest neighbors from the opposite treatment arm).\n",
    "\n",
    "We obtain $K_M$ using :class:`genriesz.KNNCatchmentBasis`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genriesz import KNNCatchmentBasis\n",
    "\n",
    "X, Y, tau, Z, D = make_ate_data(n=2000, d_z=5, seed=0)\n",
    "D_int = D.astype(int)\n",
    "\n",
    "M = 1\n",
    "idx_t = np.flatnonzero(D_int == 1)\n",
    "idx_c = np.flatnonzero(D_int == 0)\n",
    "\n",
    "# Treated weights: how many controls match to each treated unit?\n",
    "basis_t = KNNCatchmentBasis(n_neighbors=M).fit(Z[idx_t])\n",
    "Phi_ct = basis_t(Z[idx_c])\n",
    "K_t = Phi_ct.sum(axis=0)\n",
    "w_t = 1.0 + K_t / float(M)\n",
    "\n",
    "# Control weights: how many treated match to each control unit?\n",
    "basis_c = KNNCatchmentBasis(n_neighbors=M).fit(Z[idx_c])\n",
    "Phi_tc = basis_c(Z[idx_t])\n",
    "K_c = Phi_tc.sum(axis=0)\n",
    "w_c = 1.0 + K_c / float(M)\n",
    "\n",
    "w = np.empty(len(Z), dtype=float)\n",
    "w[idx_t] = w_t\n",
    "w[idx_c] = w_c\n",
    "\n",
    "scores = (2.0 * D_int - 1.0) * w * Y\n",
    "ate_hat = float(np.mean(scores))\n",
    "se = float(np.std(scores, ddof=1) / np.sqrt(len(scores)))\n",
    "\n",
    "print(\"True ATE:\", tau)\n",
    "print(f\"NN matching (M={M}) ATE:\", ate_hat)\n",
    "print(\"Naive SE:\", se)\n",
    "print(\"Naive 95% CI:\", (ate_hat - 1.96 * se, ate_hat + 1.96 * se))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc435448",
   "metadata": {},
   "source": [
    "## Notes on using non-linear learners\n",
    "\n",
    "- **Random forest / neural network as a Riesz model**:  \n",
    "  In this package the core GRR solver is **linear-in-parameters**, so the Riesz model is specified by the **basis**.  \n",
    "  To use forests / neural nets, plug them in as **fixed feature maps** (Examples 3 and 4).\n",
    "\n",
    "- **Random forest / neural network as an outcome model**:  \n",
    "  Pass any `sklearn`-style regressor as `outcome_model=...`.  \n",
    "  Use `outcome_models=\"both\"` to report AIPW/DM with both the shared linear outcome model and your custom outcome model.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}