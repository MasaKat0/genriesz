{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cd9a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.special import expit  # Sigmoid function for propensity score\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects import pandas2ri\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1ece39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027fd6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier, MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a906f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import glob\n",
    "from joblib import dump, load\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import scipy.stats\n",
    "import scipy.special\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from grr.utils.riesznet import RieszNet\n",
    "from grr.utils.moments import ate_moment_fn\n",
    "from grr.utils.ihdp_data import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b018b07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "moment_fn = ate_moment_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3e79ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_prob = 0.0  # dropout prob of dropout layers throughout notebook\n",
    "n_hidden = 100  # width of hidden layers throughout notebook\n",
    "\n",
    "# Training params\n",
    "learner_lr = 1e-5\n",
    "learner_l2 = 1e-3\n",
    "learner_l1 = 0.0\n",
    "n_epochs = 600\n",
    "earlystop_rounds = 40 # how many epochs to wait for an out-of-sample improvement\n",
    "earlystop_delta = 1e-4\n",
    "target_reg = 1.0\n",
    "riesz_weight = 0.1\n",
    "\n",
    "bs = 64\n",
    "device = torch.cuda.current_device() if torch.cuda.is_available() else None\n",
    "print(\"GPU:\", torch.cuda.is_available())\n",
    "\n",
    "from itertools import chain, combinations\n",
    "from itertools import combinations_with_replacement as combinations_w_r\n",
    "\n",
    "def _combinations(n_features, degree, interaction_only):\n",
    "        comb = (combinations if interaction_only else combinations_w_r)\n",
    "        return chain.from_iterable(comb(range(n_features), i)\n",
    "                                   for i in range(0, degree + 1))\n",
    "\n",
    "class Learner(nn.Module):\n",
    "\n",
    "    def __init__(self, n_t, n_hidden, p, degree, interaction_only=False):\n",
    "        super().__init__()\n",
    "        n_common = 200\n",
    "        self.monomials = list(_combinations(n_t, degree, interaction_only))\n",
    "        self.common = nn.Sequential(nn.Dropout(p=p), nn.Linear(n_t, n_common), nn.ELU(),\n",
    "                                    nn.Dropout(p=p), nn.Linear(n_common, n_common), nn.ELU(),\n",
    "                                    nn.Dropout(p=p), nn.Linear(n_common, n_common), nn.ELU())\n",
    "        self.riesz_nn = nn.Sequential(nn.Dropout(p=p), nn.Linear(n_common, 1))\n",
    "        self.riesz_poly = nn.Sequential(nn.Linear(len(self.monomials), 1))\n",
    "        self.reg_nn0 = nn.Sequential(nn.Dropout(p=p), nn.Linear(n_common, n_hidden), nn.ELU(),\n",
    "                                    nn.Dropout(p=p), nn.Linear(n_hidden, n_hidden), nn.ELU(),\n",
    "                                    nn.Dropout(p=p), nn.Linear(n_hidden, 1))\n",
    "        self.reg_nn1 = nn.Sequential(nn.Dropout(p=p), nn.Linear(n_common, n_hidden), nn.ELU(),\n",
    "                                    nn.Dropout(p=p), nn.Linear(n_hidden, n_hidden), nn.ELU(),\n",
    "                                    nn.Dropout(p=p), nn.Linear(n_hidden, 1))\n",
    "        self.reg_poly = nn.Sequential(nn.Linear(len(self.monomials), 1))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        poly = torch.cat([torch.prod(x[:, t], dim=1, keepdim=True)\n",
    "                          for t in self.monomials], dim=1)\n",
    "        feats = self.common(x)\n",
    "        riesz = self.riesz_nn(feats) + self.riesz_poly(poly)\n",
    "        reg = self.reg_nn0(feats) * (1 - x[:, [0]]) + self.reg_nn1(feats) * x[:, [0]] + self.reg_poly(poly)\n",
    "        return torch.cat([reg, riesz], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f572b6b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c849b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_prob = 0.0  # dropout prob of dropout layers throughout notebook\n",
    "n_hidden = 100  # width of hidden layers throughout notebook\n",
    "\n",
    "# Training params\n",
    "learner_lr = 1e-5\n",
    "learner_l2 = 1e-3\n",
    "learner_l1 = 0.0\n",
    "n_epochs = 600\n",
    "earlystop_rounds = 40 # how many epochs to wait for an out-of-sample improvement\n",
    "earlystop_delta = 1e-4\n",
    "target_reg = 1.0\n",
    "riesz_weight = 0.1\n",
    "\n",
    "bs = 64\n",
    "device = torch.cuda.current_device() if torch.cuda.is_available() else None\n",
    "print(\"GPU:\", torch.cuda.is_available())\n",
    "\n",
    "from itertools import chain, combinations\n",
    "from itertools import combinations_with_replacement as combinations_w_r\n",
    "\n",
    "def _combinations(n_features, degree, interaction_only):\n",
    "        comb = (combinations if interaction_only else combinations_w_r)\n",
    "        return chain.from_iterable(comb(range(n_features), i)\n",
    "                                   for i in range(0, degree + 1))\n",
    "\n",
    "class Learner(nn.Module):\n",
    "\n",
    "    def __init__(self, n_t, n_hidden, p, degree, interaction_only=False):\n",
    "        super().__init__()\n",
    "        n_common = 200\n",
    "        self.monomials = list(_combinations(n_t, degree, interaction_only))\n",
    "        self.common = nn.Sequential(nn.Dropout(p=p), nn.Linear(n_t, n_common), nn.ELU(),\n",
    "                                    nn.Dropout(p=p), nn.Linear(n_common, n_common), nn.ELU(),\n",
    "                                    nn.Dropout(p=p), nn.Linear(n_common, n_common), nn.ELU())\n",
    "        self.riesz_nn = nn.Sequential(nn.Dropout(p=p), nn.Linear(n_common, 1))\n",
    "        self.riesz_poly = nn.Sequential(nn.Linear(len(self.monomials), 1))\n",
    "        self.reg_nn0 = nn.Sequential(nn.Dropout(p=p), nn.Linear(n_common, n_hidden), nn.ELU(),\n",
    "                                    nn.Dropout(p=p), nn.Linear(n_hidden, n_hidden), nn.ELU(),\n",
    "                                    nn.Dropout(p=p), nn.Linear(n_hidden, 1))\n",
    "        self.reg_nn1 = nn.Sequential(nn.Dropout(p=p), nn.Linear(n_common, n_hidden), nn.ELU(),\n",
    "                                    nn.Dropout(p=p), nn.Linear(n_hidden, n_hidden), nn.ELU(),\n",
    "                                    nn.Dropout(p=p), nn.Linear(n_hidden, 1))\n",
    "        self.reg_poly = nn.Sequential(nn.Linear(len(self.monomials), 1))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        poly = torch.cat([torch.prod(x[:, t], dim=1, keepdim=True)\n",
    "                          for t in self.monomials], dim=1)\n",
    "        feats = self.common(x)\n",
    "        riesz = self.riesz_nn(feats) + self.riesz_poly(poly)\n",
    "        reg = self.reg_nn0(feats) * (1 - x[:, [0]]) + self.reg_nn1(feats) * x[:, [0]] + self.reg_poly(poly)\n",
    "        return torch.cat([reg, riesz], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e11ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation parameters\n",
    "n = 3000 # Number of samples\n",
    "p = 3   # Number of covariates\n",
    "treatment_effect = 5.0  # True treatment effect\n",
    "\n",
    "# Generate covariates\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa011270",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_ATE = treatment_effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a75ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "class NeuralNetBiasCorrection:\n",
    "    def __init__(self, input_dim, hidden_dim=100, max_iter=3000, tol=1e-10, lbd=0.01, loss=\"Logit\", lr=0.01):\n",
    "        \"\"\"\n",
    "        Bias-correction model using a neural network\n",
    "        :param input_dim: Dimension of input features\n",
    "        :param hidden_dim: Number of hidden units in the hidden layer\n",
    "        :param max_iter: Maximum number of iterations (epochs)\n",
    "        :param tol: Convergence tolerance\n",
    "        :param lbd: Weight of the regularization term\n",
    "        :param loss: Loss function type (e.g., \"Logit\", \"DBCLS\", \"DBCLogit\")\n",
    "        :param lr: Learning rate\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.lbd = lbd\n",
    "        self.lr = lr\n",
    "        \n",
    "        # Build the neural network\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        if loss == \"Logit\":\n",
    "            self.criterion = self._logistic_loss_function\n",
    "        elif loss == \"CBPS\":\n",
    "            self.criterion = self._cbps_loss_function\n",
    "        elif loss == \"DBCLS\":\n",
    "            self.criterion = self._least_squares_loss_function\n",
    "        elif loss == \"DBCUKL\":\n",
    "            self.criterion = self._ukl_loss_function\n",
    "        elif loss == \"DBCEB\":\n",
    "            self.criterion = self._eb_loss_function\n",
    "        else:\n",
    "            raise ValueError(\"Invalid loss function specified\")\n",
    "\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        \n",
    "    \n",
    "    def _least_squares_loss_function(self, X_tensor, outputs, targets):\n",
    "        \"\"\"Least-squares loss function.\"\"\"\n",
    "        outputs = torch.clamp(outputs, min=0.05, max=0.95)\n",
    "        loss = torch.mean(-2 * (1 / outputs + 1 / (1 - outputs)) + (targets / outputs - (1 - targets) / (1 - outputs))**2)\n",
    "        return loss + self.lbd * self._l2_regularization()\n",
    "    \n",
    "    def _ukl_loss_function(self, X_tensor, outputs, targets):\n",
    "        \"\"\"Constrained logistic-style loss function.\"\"\"\n",
    "        outputs = torch.clamp(outputs, min=0.05, max=0.95)\n",
    "        # Correcting the formula with the proper variable names\n",
    "        loss = - torch.log(1/outputs) - torch.log(1/(1 - outputs)) + targets / outputs + (1 - targets) / (1 - outputs)\n",
    "        # Mean loss calculation\n",
    "        loss = torch.mean(loss)\n",
    "        return loss + self.lbd * self._l2_regularization()\n",
    "    \n",
    "    def _eb_loss_function(self, X_tensor, outputs, targets):\n",
    "        \"\"\"Constrained logistic-style loss function.\"\"\"\n",
    "        outputs = torch.clamp(outputs, min=0.05, max=0.95)\n",
    "        # Correcting the formula with the proper variable names\n",
    "        loss = - (1 - targets)*torch.log(1/outputs - 1) - targets*torch.log(1/(1 - outputs) - 1) + targets*(1/outputs) + (1 - targets)*(1/(1 - outputs))\n",
    "        # Mean loss calculation\n",
    "        loss = torch.mean(loss)\n",
    "        return loss + self.lbd * self._l2_regularization()\n",
    "    \n",
    "    def _logistic_loss_function(self, X_tensor, outputs, targets):\n",
    "        \"\"\"Logistic loss function.\"\"\"\n",
    "        outputs = torch.clamp(outputs, min=0.05, max=0.95)\n",
    "        loss = nn.BCELoss()(outputs, targets)  # Binary cross-entropy\n",
    "        return loss + self.lbd * self._l2_regularization()\n",
    "    \n",
    "    def _cbps_loss_function(self, X_tensor, outputs, targets):\n",
    "        \"\"\"Logistic loss function.\"\"\"\n",
    "        outputs = torch.clamp(outputs, min=0.05, max=0.95)\n",
    "        loss =  torch.mean((targets * X_tensor / outputs - (1 - targets) * X_tensor / (1 - outputs))**2, axis=0)\n",
    "        loss = (loss**2).mean()\n",
    "        return loss + self.lbd * self._l2_regularization()\n",
    "\n",
    "    def _l2_regularization(self):\n",
    "        \"\"\"Compute the L2 regularization term.\"\"\"\n",
    "        reg_loss = sum(torch.sum(param**2) for param in self.model.parameters())\n",
    "        return reg_loss\n",
    "    \n",
    "    def fit(self, X, T, batch_size=100):\n",
    "        \"\"\"\n",
    "        Train the model (mini-batch training)\n",
    "        :param X: Covariates (array of shape N×d)\n",
    "        :param T: Targets (binary array of shape N×1)\n",
    "        :param batch_size: Mini-batch size\n",
    "        \"\"\"\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "        T_tensor = torch.tensor(T, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "        dataset = TensorDataset(X_tensor, T_tensor)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        prev_loss = float('inf')\n",
    "        for epoch in range(self.max_iter):\n",
    "            for X_batch, T_batch in dataloader:\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(X_batch)\n",
    "                loss = self.criterion(X_batch, outputs, T_batch)  # Use the mini-batch inputs (X_batch) when evaluating the loss\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            # Convergence check (per epoch)\n",
    "            current_loss = loss.item()\n",
    "            if abs(prev_loss - current_loss) < self.tol:\n",
    "                break\n",
    "            prev_loss = current_loss\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Compute predicted probabilities\n",
    "        :param X: Covariates (array of shape N×d)\n",
    "        :return: Probability of class 1\n",
    "        \"\"\"\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            probas = torch.clamp(self.model(X_tensor), min=0.05, max=0.95).numpy()\n",
    "        return np.hstack([1 - probas, probas])\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels\n",
    "        :param X: Covariates (array of shape N×d)\n",
    "        :return: Predicted class (0 or 1)\n",
    "        \"\"\"\n",
    "        probas = self.predict_proba(X)\n",
    "        return (probas[:, 1] >= 0.5).astype(int)\n",
    "\n",
    "    def get_params(self):\n",
    "        \"\"\"\n",
    "        Get trained parameters\n",
    "        :return: Model weights and biases\n",
    "        \"\"\"\n",
    "        params = {name: param.detach().numpy() for name, param in self.model.named_parameters()}\n",
    "        return params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62df913a",
   "metadata": {},
   "source": [
    "### Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac5f325",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trial = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fe2342",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsims = 1000\n",
    "np.random.seed(123)\n",
    "#sim_ids = np.random.choice(len(simulation_files), nsims, replace=False)\n",
    "methods = ['dr', 'direct', 'ips']\n",
    "srr = {'dr' : True, 'direct' : False, 'ips' : True}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3887e37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = []\n",
    "\n",
    "for tr in range(num_trial):\n",
    "    print(tr)\n",
    "    result_list_temp = []\n",
    "    \n",
    "    X = np.random.normal(0, 1, (n, p))\n",
    "\n",
    "    # Define a propensity score model\n",
    "    # Assume treatment probability is a sigmoid function of a subset of covariates\n",
    "    X_temp = np.concatenate([X, X**2, np.array([X[:, 0]*X[:, 1], X[:, 1]*X[:, 2], X[:, 0]*X[:, 2]]).T], axis=1)\n",
    "    propensity_coef = np.random.normal(0, 0.5, X_temp.shape[1])\n",
    "    propensity_scores = expit(X_temp @ propensity_coef)  # Calculate propensity scores\n",
    "\n",
    "    # Generate treatment assignment based on propensity scores\n",
    "    T = np.random.binomial(1, propensity_scores)\n",
    "\n",
    "    # Generate outcome with treatment effect\n",
    "    # Assume a simple linear model for demonstration\n",
    "    beta = np.random.normal(0, 1, p)\n",
    "    Y = (X @ beta)**2 + 1.1 + treatment_effect * T + np.random.normal(0, 1, n)\n",
    "\n",
    "    X_treatment = X[T == 1]\n",
    "    X_control = X[T == 0]\n",
    "    \n",
    "    y_scaler = StandardScaler(with_mean=True).fit(np.array([Y]).T)\n",
    "    y = y_scaler.transform(np.array([Y]).T)\n",
    "    XT = np.c_[T, X]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(XT, y, test_size = 0.2)\n",
    "                            \n",
    "    Y = y.T[0]\n",
    "    \n",
    "    Y_treatment = Y[T == 1]\n",
    "    Y_control = Y[T == 0]\n",
    "    \n",
    "    XT = np.c_[T, X]\n",
    "    \n",
    "    outcome_model = MLPRegressor(random_state=1, max_iter=600)\n",
    "    outcome_model.fit(XT, Y)\n",
    "\n",
    "    T_treatment = T*0 + 1\n",
    "    T_control = T*0\n",
    "    XT_treatment = np.c_[T_treatment, X]\n",
    "    XT_control = np.c_[T_control, X]\n",
    "\n",
    "    est_treatment_outcome = outcome_model.predict(XT_treatment)\n",
    "    est_control_outcome = outcome_model.predict(XT_control)\n",
    "    \n",
    "    for method in [\"DBCLS\", \"DBCUKL\", \"DBCEB\", \"Logit\"]:\n",
    "        prop_model = NeuralNetBiasCorrection(input_dim=p, lbd = 0., loss=method)\n",
    "        prop_model.fit(X, T)\n",
    "        est_prop_score = prop_model.predict_proba(X)[:, 1]\n",
    "        est_prop_score_dbc = est_prop_score\n",
    "\n",
    "        #treatment_outcome_model = KernelRegression(kernel=\"rbf\", gamma=np.logspace(-2, 2, 10))\n",
    "        #control_outcome_model = KernelRegression(kernel=\"rbf\", gamma=np.logspace(-2, 2, 10))\n",
    "\n",
    "        IPW_est = np.mean(T*Y / est_prop_score - (1 - T)*Y / (1 - est_prop_score))\n",
    "\n",
    "        # Evaluate performance\n",
    "        IPW_bias = IPW_est - true_ATE\n",
    "\n",
    "        result_list_temp.append(IPW_est)\n",
    "\n",
    "        DM_est = np.mean(est_treatment_outcome - est_control_outcome)\n",
    "\n",
    "        # Evaluate performance\n",
    "        DM_bias = DM_est - true_ATE\n",
    "        \n",
    "        result_list_temp.append(DM_est)\n",
    "\n",
    "        DR_est = np.mean(T*(Y - est_treatment_outcome) / est_prop_score - (1 - T)*(Y - est_control_outcome)  / (1 - est_prop_score) + est_treatment_outcome - est_control_outcome)\n",
    "\n",
    "        # Evaluate performance\n",
    "        DR_bias = DR_est - true_ATE\n",
    "\n",
    "        result_list_temp.append(DR_est)\n",
    "\n",
    "    result_list.append(result_list_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd82cdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"result\", result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0001d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.array(result_list)**2, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8bdb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(result_list).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add33bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = tuple(np.array(x) for x in zip(*result_list2))\n",
    "truth = res[-1:]\n",
    "res_dict = {}\n",
    "\n",
    "res_list_temp = []\n",
    "for it, method in enumerate(methods):\n",
    "    point, lb, ub = res[it * 3: (it + 1)*3]\n",
    "    res_list_temp.append(point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d1fa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list_final = np.concatenate([np.array(result_list), np.array(res_list_temp).T], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6ba87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"ATE_result1.csv\", result_list_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210eae79",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(np.sqrt(np.mean((result_list_final - true_ATE)**2, axis=0)), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eaae506",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean((np.array(res_list_temp).T - true_ATE)**2, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b735a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f29bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = tuple(np.array(x) for x in zip(*result_list2))\n",
    "truth = res[-1:]\n",
    "res_dict = {}\n",
    "\n",
    "res_list_temp = []\n",
    "for it, method in enumerate(methods):\n",
    "    point, lb, ub = res[it * 3: (it + 1)*3]\n",
    "    res_list_temp.append(point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e959b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate([np.array(result_list), np.array(res_list_temp).T], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f12aa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(res_list_temp).T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ffad3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497497c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(result_list).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6a367a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd6435b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898003c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be022644",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f70088",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681a2551",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = []\n",
    "\n",
    "for tr in range(num_trial):\n",
    "    result_list_temp = []\n",
    "    \n",
    "    \n",
    "    X = np.random.normal(0, 1, (n, p))\n",
    "\n",
    "    # Define a propensity score model\n",
    "    # Assume treatment probability is a sigmoid function of a subset of covariates\n",
    "    X_temp = np.concatenate([X, X**2, np.array([X[:, 0]*X[:, 1], X[:, 1]*X[:, 2], X[:, 0]*X[:, 2]]).T], axis=1)\n",
    "    propensity_coef = np.random.normal(0, 0.5, X_temp.shape[1])\n",
    "    propensity_scores = expit(X_temp @ propensity_coef)  # Calculate propensity scores\n",
    "\n",
    "    # Generate treatment assignment based on propensity scores\n",
    "    T = np.random.binomial(1, propensity_scores)\n",
    "\n",
    "    # Generate outcome with treatment effect\n",
    "    # Assume a simple linear model for demonstration\n",
    "    beta = np.random.normal(0, 1, p)\n",
    "    Y = (X @ beta)**2 + 1.1 + treatment_effect * T + np.random.normal(0, 1, n)\n",
    "\n",
    "    X_treatment = X[T == 1]\n",
    "    X_control = X[T == 0]\n",
    "\n",
    "    Y_treatment = Y[T == 1]\n",
    "    Y_control = Y[T == 0]\n",
    "    \n",
    "    #### Direct bias correction\n",
    "    prop_model = NeuralNetBiasCorrection(input_dim=p, lbd = 0.01, loss=\"DBCLS\")\n",
    "    prop_model.fit(X, T)\n",
    "    est_prop_score = prop_model.predict_proba(X)[:, 1]\n",
    "    est_prop_score_dbc = est_prop_score\n",
    "\n",
    "    treatment_outcome_model = KernelRegression(kernel=\"rbf\", gamma=np.logspace(-2, 2, 10))\n",
    "    control_outcome_model = KernelRegression(kernel=\"rbf\", gamma=np.logspace(-2, 2, 10))\n",
    "\n",
    "    treatment_outcome_model.fit(X_treatment, Y_treatment)\n",
    "    control_outcome_model.fit(X_control, Y_control)\n",
    "\n",
    "    est_treatment_outcome = treatment_outcome_model.predict(X)\n",
    "    est_control_outcome = control_outcome_model.predict(X)\n",
    "\n",
    "    IPW_est = np.mean(T*Y / est_prop_score - (1 - T)*Y / (1 - est_prop_score))\n",
    "\n",
    "    # Evaluate performance\n",
    "    IPW_bias = IPW_est - true_ATE\n",
    "\n",
    "    print(f\"Estimated ATE: {IPW_est}\")\n",
    "    print(f\"Bias: {IPW_bias}\")\n",
    "    \n",
    "    result_list_temp.append(IPW_est)\n",
    "    \n",
    "    DM_est = np.mean(est_treatment_outcome - est_control_outcome)\n",
    "\n",
    "    # Evaluate performance\n",
    "    DM_bias = DM_est - true_ATE\n",
    "\n",
    "    print(f\"Estimated ATE: {DM_est}\")\n",
    "    print(f\"Bias: {DM_bias}\")\n",
    "    \n",
    "    result_list_temp.append(DM_est)\n",
    "\n",
    "    DR_est = np.mean(T*(Y - est_treatment_outcome) / est_prop_score - (1 - T)*(Y - est_control_outcome)  / (1 - est_prop_score) + est_treatment_outcome - est_control_outcome)\n",
    "\n",
    "    # Evaluate performance\n",
    "    DR_bias = DR_est - true_ATE\n",
    "\n",
    "    print(f\"Estimated ATE: {DR_est}\")\n",
    "    print(f\"Bias: {DR_bias}\")\n",
    "    \n",
    "    result_list_temp.append(DR_est)\n",
    "    \n",
    "    #### Direct bias correction\n",
    "    prop_model = NeuralNetBiasCorrection(input_dim=p, lbd = 0.01, loss=\"CBPS\")\n",
    "    prop_model.fit(X, T)\n",
    "    est_prop_score = prop_model.predict_proba(X)[:, 1]\n",
    "    est_prop_score_dbc = est_prop_score\n",
    "\n",
    "    treatment_outcome_model = KernelRegression(kernel=\"rbf\", gamma=np.logspace(-2, 2, 10))\n",
    "    control_outcome_model = KernelRegression(kernel=\"rbf\", gamma=np.logspace(-2, 2, 10))\n",
    "\n",
    "    treatment_outcome_model.fit(X_treatment, Y_treatment)\n",
    "    control_outcome_model.fit(X_control, Y_control)\n",
    "\n",
    "    est_treatment_outcome = treatment_outcome_model.predict(X)\n",
    "    est_control_outcome = control_outcome_model.predict(X)\n",
    "\n",
    "    IPW_est = np.mean(T*Y / est_prop_score - (1 - T)*Y / (1 - est_prop_score))\n",
    "\n",
    "    # Evaluate performance\n",
    "    IPW_bias = IPW_est - true_ATE\n",
    "\n",
    "    print(f\"Estimated ATE: {IPW_est}\")\n",
    "    print(f\"Bias: {IPW_bias}\")\n",
    "    \n",
    "    result_list_temp.append(IPW_est)\n",
    "    \n",
    "    DM_est = np.mean(est_treatment_outcome - est_control_outcome)\n",
    "\n",
    "    # Evaluate performance\n",
    "    DM_bias = DM_est - true_ATE\n",
    "\n",
    "    print(f\"Estimated ATE: {DM_est}\")\n",
    "    print(f\"Bias: {DM_bias}\")\n",
    "    \n",
    "    result_list_temp.append(DM_est)\n",
    "\n",
    "    DR_est = np.mean(T*(Y - est_treatment_outcome) / est_prop_score - (1 - T)*(Y - est_control_outcome)  / (1 - est_prop_score) + est_treatment_outcome - est_control_outcome)\n",
    "\n",
    "    # Evaluate performance\n",
    "    DR_bias = DR_est - true_ATE\n",
    "\n",
    "    print(f\"Estimated ATE: {DR_est}\")\n",
    "    print(f\"Bias: {DR_bias}\")\n",
    "    \n",
    "    result_list_temp.append(DR_est)\n",
    "    \n",
    "    ##### Linear models\n",
    "    \n",
    "    # Fit a linear model to estimate the treatment effect\n",
    "    model = LinearRegression()\n",
    "    model.fit(np.hstack([X, T.reshape(-1, 1)]), Y)\n",
    "    estimated_treatment_effect = model.coef_[-1]\n",
    "\n",
    "    # Evaluate performance\n",
    "    true_ATE = treatment_effect\n",
    "    bias = estimated_treatment_effect - true_ATE\n",
    "    mse = mean_squared_error(Y, model.predict(np.hstack([X, T.reshape(-1, 1)])))\n",
    "\n",
    "    print(f\"Estimated ATE: {estimated_treatment_effect}\")\n",
    "    print(f\"Bias: {bias}\")\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "    result_list_temp.append(estimated_treatment_effect)\n",
    "    \n",
    "    #### Logistc regression \n",
    "    \n",
    "    prop_model = NeuralNetBiasCorrection(input_dim=p, lbd = 0., loss=\"Logit\")\n",
    "    prop_model.fit(X, T)\n",
    "    est_prop_score = prop_model.predict_proba(X)[:, 1]\n",
    "\n",
    "    treatment_outcome_model = KernelRegression(kernel=\"rbf\", gamma=np.logspace(-2, 2, 10))\n",
    "    control_outcome_model = KernelRegression(kernel=\"rbf\", gamma=np.logspace(-2, 2, 10))\n",
    "\n",
    "    treatment_outcome_model.fit(X_treatment, Y_treatment)\n",
    "    control_outcome_model.fit(X_control, Y_control)\n",
    "\n",
    "    est_treatment_outcome = treatment_outcome_model.predict(X)\n",
    "    est_control_outcome = control_outcome_model.predict(X)\n",
    "\n",
    "    IPW_est = np.mean(T*Y / est_prop_score - (1 - T)*Y / (1 - est_prop_score))\n",
    "\n",
    "    # Evaluate performance\n",
    "    IPW_bias = IPW_est - true_ATE\n",
    "\n",
    "    print(f\"Estimated ATE: {IPW_est}\")\n",
    "    print(f\"Bias: {IPW_bias}\")\n",
    "    \n",
    "    result_list_temp.append(IPW_est)\n",
    "    \n",
    "    DM_est = np.mean(est_treatment_outcome - est_control_outcome)\n",
    "\n",
    "    # Evaluate performance\n",
    "    DM_bias = DM_est - true_ATE\n",
    "\n",
    "    print(f\"Estimated ATE: {DM_est}\")\n",
    "    print(f\"Bias: {DM_bias}\")\n",
    "    \n",
    "    result_list_temp.append(DM_est)\n",
    "\n",
    "    DR_est = np.mean(T*(Y - est_treatment_outcome) / est_prop_score - (1 - T)*(Y - est_control_outcome)  / (1 - est_prop_score) + est_treatment_outcome - est_control_outcome)\n",
    "\n",
    "    # Evaluate performance\n",
    "    DR_bias = DR_est - true_ATE\n",
    "\n",
    "    print(f\"Estimated ATE: {DR_est}\")\n",
    "    print(f\"Bias: {DR_bias}\")\n",
    "    \n",
    "    result_list_temp.append(DR_est)\n",
    "    \n",
    "    #### CBPS\n",
    "    \n",
    "    # Enable automatic conversion of Pandas DataFrame to R DataFrame\n",
    "    pandas2ri.activate()\n",
    "\n",
    "    # Simulate data in Python\n",
    "\n",
    "    # Create a pandas DataFrame\n",
    "    column_names = [f'X{i+1}' for i in range(p)]\n",
    "    df = pd.DataFrame(X, columns=column_names)\n",
    "    df['T'] = T\n",
    "    df['Y'] = Y\n",
    "\n",
    "\n",
    "    # Convert pandas DataFrame to R DataFrame\n",
    "    r_df = pandas2ri.py2rpy(df)\n",
    "\n",
    "    ro.r.assign(\"p\", p)\n",
    "\n",
    "    # Load the CBPS package in R and fit the model for ATE estimation\n",
    "    ro.r('''\n",
    "        library(CBPS)\n",
    "        estimate_cbps_ate <- function(df) {\n",
    "            formula_str <- paste(\"T ~\", paste(names(df)[1:{p}], collapse=\" + \"))\n",
    "\n",
    "            # Apply CBPS (estimate the ATE, ATT=0)\n",
    "            model <- CBPS(as.formula(formula_str), data = df, ATT = 0, method = \"exact\")\n",
    "\n",
    "            # Retrieve the estimated propensity scores\n",
    "            df$propensity_score <- fitted(model)\n",
    "\n",
    "            # Apply IPW (Inverse Probability Weighting)\n",
    "            df$weight <- ifelse(df$T == 1, 1 / df$propensity_score, 1 / (1 - df$propensity_score))\n",
    "\n",
    "            # Estimate the ATE via weighted regression\n",
    "            result <- lm(Y ~ T, data = df, weights = df$weight)\n",
    "\n",
    "            return(df$propensity_score)\n",
    "        }\n",
    "    ''')\n",
    "\n",
    "    # Call the R function to obtain propensity scores (and the ATE)\n",
    "    est_prop_score = ro.r['estimate_cbps_ate'](r_df)\n",
    "    \n",
    "    est_prop_score_cbps = est_prop_score\n",
    "    \n",
    "    #print(er)\n",
    "\n",
    "    treatment_outcome_model = KernelRegression(kernel=\"rbf\", gamma=np.logspace(-2, 2, 10))\n",
    "    control_outcome_model = KernelRegression(kernel=\"rbf\", gamma=np.logspace(-2, 2, 10))\n",
    "\n",
    "    treatment_outcome_model.fit(X_treatment, Y_treatment)\n",
    "    control_outcome_model.fit(X_control, Y_control)\n",
    "\n",
    "    est_treatment_outcome = treatment_outcome_model.predict(X)\n",
    "    est_control_outcome = control_outcome_model.predict(X)\n",
    "\n",
    "    IPW_est = np.mean(T*Y / est_prop_score - (1 - T)*Y / (1 - est_prop_score))\n",
    "\n",
    "    # Evaluate performance\n",
    "    IPW_bias = IPW_est - true_ATE\n",
    "\n",
    "    print(f\"Estimated ATE: {IPW_est}\")\n",
    "    print(f\"Bias: {IPW_bias}\")\n",
    "    \n",
    "    result_list_temp.append(IPW_est)\n",
    "    \n",
    "    DM_est = np.mean(est_treatment_outcome - est_control_outcome)\n",
    "\n",
    "    # Evaluate performance\n",
    "    DM_bias = DM_est - true_ATE\n",
    "\n",
    "    print(f\"Estimated ATE: {DM_est}\")\n",
    "    print(f\"Bias: {DM_bias}\")\n",
    "    \n",
    "    result_list_temp.append(DM_est)\n",
    "\n",
    "    DR_est = np.mean(T*(Y - est_treatment_outcome) / est_prop_score - (1 - T)*(Y - est_control_outcome)  / (1 - est_prop_score) + est_treatment_outcome - est_control_outcome)\n",
    "\n",
    "    # Evaluate performance\n",
    "    DR_bias = DR_est - true_ATE\n",
    "\n",
    "    print(f\"Estimated ATE: {DR_est}\")\n",
    "    print(f\"Bias: {DR_bias}\")\n",
    "    \n",
    "    result_list_temp.append(DR_est)\n",
    "    \n",
    "    \n",
    "    result_list.append(result_list_temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877587bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(np.sqrt(np.mean((np.array(result_list) - true_ATE)**2, axis=0)), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c15208e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe34dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = tuple(np.array(x) for x in zip(*result_list2))\n",
    "truth = res[-1:]\n",
    "res_dict = {}\n",
    "for it, method in enumerate(methods):\n",
    "    point, lb, ub = res[it * 3: (it + 1)*3]\n",
    "    res_dict[method] = {'point': point, 'lb': lb, 'ub': ub,\n",
    "                        'cov': np.mean(np.logical_and(truth >= lb, truth <= ub)),\n",
    "                        'bias': np.mean(point - truth),\n",
    "                        'rmse': rmse_fn(point, truth)\n",
    "                        }\n",
    "    print(\"{} : bias = {:.3f}, rmse = {:.3f}, cov = {:.3f}\".format(method, res_dict[method]['bias'], res_dict[method]['rmse'], res_dict[method]['cov']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50178102",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(np.sqrt(np.mean((np.array(result_list) - true_ATE)**2, axis=0)), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e450fc71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee580a3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc127466",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate([X, X**2], axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387bdcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9951e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(propensity_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a7166b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(propensity_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fc6b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(est_prop_score_dbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1294d0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(est_prop_score_dbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becdedb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(est_prop_score_cbps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03707905",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(est_prop_score_cbps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c2fc0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8160fbf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14358bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2698c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(np.sqrt(np.mean((np.array(result_list) - true_ATE)**2, axis=0)), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e68cba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0c3be6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b0e17d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb0d934",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"True Average Treatment Effect (ATE): {true_ATE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9d0835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a linear model to estimate the treatment effect\n",
    "model = LinearRegression()\n",
    "model.fit(np.hstack([X, T.reshape(-1, 1)]), Y)\n",
    "estimated_treatment_effect = model.coef_[-1]\n",
    "\n",
    "# Evaluate performance\n",
    "true_ATE = treatment_effect\n",
    "bias = estimated_treatment_effect - true_ATE\n",
    "mse = mean_squared_error(Y, model.predict(np.hstack([X, T.reshape(-1, 1)])))\n",
    "\n",
    "print(f\"Estimated ATE: {estimated_treatment_effect}\")\n",
    "print(f\"Bias: {bias}\")\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f6b42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a linear model to estimate the treatment effect\n",
    "prop_model = LogisticRegression()\n",
    "prop_model.fit(X, T)\n",
    "est_prop_score = prop_model.predict_proba(X)[:, 1]\n",
    "\n",
    "treatment_outcome_model = KernelRegression(kernel=\"rbf\", gamma=np.logspace(-2, 2, 10))\n",
    "control_outcome_model = KernelRegression(kernel=\"rbf\", gamma=np.logspace(-2, 2, 10))\n",
    "\n",
    "treatment_outcome_model.fit(X_treatment, Y_treatment)\n",
    "control_outcome_model.fit(X_control, Y_control)\n",
    "\n",
    "est_treatment_outcome = treatment_outcome_model.predict(X)\n",
    "est_control_outcome = control_outcome_model.predict(X)\n",
    "\n",
    "IPW_est = np.mean(T*Y / est_prop_score - (1 - T)*Y / (1 - est_prop_score))\n",
    "\n",
    "# Evaluate performance\n",
    "IPW_bias = IPW_est - true_ATE\n",
    "\n",
    "print(f\"Estimated ATE: {IPW_est}\")\n",
    "print(f\"Bias: {IPW_bias}\")\n",
    "\n",
    "DR_est = np.mean(T*(Y - est_treatment_outcome) / est_prop_score - (1 - T)*(Y - est_control_outcome)  / (1 - est_prop_score) + est_treatment_outcome - est_control_outcome)\n",
    "\n",
    "# Evaluate performance\n",
    "DR_bias = DR_est - true_ATE\n",
    "\n",
    "print(f\"Estimated ATE: {DR_est}\")\n",
    "print(f\"Bias: {DR_bias}\")\n",
    "\n",
    "DM_est = np.mean(est_treatment_outcome - est_control_outcome)\n",
    "\n",
    "# Evaluate performance\n",
    "DM_bias = DM_est - true_ATE\n",
    "\n",
    "print(f\"Estimated ATE: {DM_est}\")\n",
    "print(f\"Bias: {DM_bias}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e4a042",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                              result_list = []\n",
    "\n",
    "for tr in range(num_trial):\n",
    "    result_list_temp = []\n",
    "    \n",
    "    \n",
    "    X = np.random.normal(0, 1, (n, p))\n",
    "\n",
    "    # Define a propensity score model\n",
    "    # Assume treatment probability is a sigmoid function of a subset of covariates\n",
    "    propensity_coef = np.random.normal(0, 0.5, p)\n",
    "    propensity_scores = expit(X @ propensity_coef)  # Calculate propensity scores\n",
    "\n",
    "    # Generate treatment assignment based on propensity scores\n",
    "    T = np.random.binomial(1, propensity_scores)\n",
    "\n",
    "    # Generate outcome with treatment effect\n",
    "    # Assume a simple linear model for demonstration\n",
    "    beta = np.random.normal(0, 1, p)\n",
    "    Y = (X @ beta)**2 + 1.1 + treatment_effect * T + np.random.normal(0, 1, n)\n",
    "\n",
    "    X_treatment = X[T == 1]\n",
    "    X_control = X[T == 0]\n",
    "\n",
    "    Y_treatment = Y[T == 1]\n",
    "    Y_control = Y[T == 0]\n",
    "    \n",
    "        #### Direct bias correction\n",
    "    prop_model = DirectBiasCorrection(lbd = 0.01)\n",
    "    \n",
    "    prop_model.fit(X, T, Y)\n",
    "    est_prop_score = prop_model.predict_proba(X)[:, 1]\n",
    "    est_prop_score_dbc = est_prop_score\n",
    "\n",
    "    treatment_outcome_model = KernelRegression(kernel=\"rbf\", gamma=np.logspace(-2, 2, 10))\n",
    "    control_outcome_model = KernelRegression(kernel=\"rbf\", gamma=np.logspace(-2, 2, 10))\n",
    "\n",
    "    treatment_outcome_model.fit(X_treatment, Y_treatment)\n",
    "    control_outcome_model.fit(X_control, Y_control)\n",
    "\n",
    "    est_treatment_outcome = treatment_outcome_model.predict(X)\n",
    "    est_control_outcome = control_outcome_model.predict(X)\n",
    "\n",
    "    IPW_est = np.mean(T*Y / est_prop_score - (1 - T)*Y / (1 - est_prop_score))\n",
    "\n",
    "    # Evaluate performance\n",
    "    IPW_bias = IPW_est - true_ATE\n",
    "\n",
    "    print(f\"Estimated ATE: {IPW_est}\")\n",
    "    print(f\"Bias: {IPW_bias}\")\n",
    "    \n",
    "    result_list_temp.append(IPW_est)\n",
    "    \n",
    "    DM_est = np.mean(est_treatment_outcome - est_control_outcome)\n",
    "\n",
    "    # Evaluate performance\n",
    "    DM_bias = DM_est - true_ATE\n",
    "\n",
    "    print(f\"Estimated ATE: {DM_est}\")\n",
    "    print(f\"Bias: {DM_bias}\")\n",
    "    \n",
    "    result_list_temp.append(DM_est)\n",
    "\n",
    "    DR_est = np.mean(T*(Y - est_treatment_outcome) / est_prop_score - (1 - T)*(Y - est_control_outcome)  / (1 - est_prop_score) + est_treatment_outcome - est_control_outcome)\n",
    "\n",
    "    # Evaluate performance\n",
    "    DR_bias = DR_est - true_ATE\n",
    "\n",
    "    print(f\"Estimated ATE: {DR_est}\")\n",
    "    print(f\"Bias: {DR_bias}\")\n",
    "    \n",
    "    result_list_temp.append(DR_est)\n",
    "    \n",
    "    ##### Linear models\n",
    "    \n",
    "    # Fit a linear model to estimate the treatment effect\n",
    "    model = LinearRegression()\n",
    "    model.fit(np.hstack([X, T.reshape(-1, 1)]), Y)\n",
    "    estimated_treatment_effect = model.coef_[-1]\n",
    "\n",
    "    # Evaluate performance\n",
    "    true_ATE = treatment_effect\n",
    "    bias = estimated_treatment_effect - true_ATE\n",
    "    mse = mean_squared_error(Y, model.predict(np.hstack([X, T.reshape(-1, 1)])))\n",
    "\n",
    "    print(f\"Estimated ATE: {estimated_treatment_effect}\")\n",
    "    print(f\"Bias: {bias}\")\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "    result_list_temp.append(estimated_treatment_effect)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #### Logistc regression \n",
    "    \n",
    "    prop_model = LogisticRegression()\n",
    "    prop_model.fit(X, T)\n",
    "    est_prop_score = prop_model.predict_proba(X)[:, 1]\n",
    "\n",
    "    treatment_outcome_model = KernelRegression(kernel=\"rbf\", gamma=np.logspace(-2, 2, 10))\n",
    "    control_outcome_model = KernelRegression(kernel=\"rbf\", gamma=np.logspace(-2, 2, 10))\n",
    "\n",
    "    treatment_outcome_model.fit(X_treatment, Y_treatment)\n",
    "    control_outcome_model.fit(X_control, Y_control)\n",
    "\n",
    "    est_treatment_outcome = treatment_outcome_model.predict(X)\n",
    "    est_control_outcome = control_outcome_model.predict(X)\n",
    "\n",
    "    IPW_est = np.mean(T*Y / est_prop_score - (1 - T)*Y / (1 - est_prop_score))\n",
    "\n",
    "    # Evaluate performance\n",
    "    IPW_bias = IPW_est - true_ATE\n",
    "\n",
    "    print(f\"Estimated ATE: {IPW_est}\")\n",
    "    print(f\"Bias: {IPW_bias}\")\n",
    "    \n",
    "    result_list_temp.append(IPW_est)\n",
    "    \n",
    "    DM_est = np.mean(est_treatment_outcome - est_control_outcome)\n",
    "\n",
    "    # Evaluate performance\n",
    "    DM_bias = DM_est - true_ATE\n",
    "\n",
    "    print(f\"Estimated ATE: {DM_est}\")\n",
    "    print(f\"Bias: {DM_bias}\")\n",
    "    \n",
    "    result_list_temp.append(DM_est)\n",
    "\n",
    "    DR_est = np.mean(T*(Y - est_treatment_outcome) / est_prop_score - (1 - T)*(Y - est_control_outcome)  / (1 - est_prop_score) + est_treatment_outcome - est_control_outcome)\n",
    "\n",
    "    # Evaluate performance\n",
    "    DR_bias = DR_est - true_ATE\n",
    "\n",
    "    print(f\"Estimated ATE: {DR_est}\")\n",
    "    print(f\"Bias: {DR_bias}\")\n",
    "    \n",
    "    result_list_temp.append(DR_est)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    #### CBPS\n",
    "    \n",
    "    # Enable automatic conversion of Pandas DataFrame to R DataFrame\n",
    "    pandas2ri.activate()\n",
    "\n",
    "    # Simulate data in Python\n",
    "\n",
    "    # Create a pandas DataFrame\n",
    "    column_names = [f'X{i+1}' for i in range(p)]\n",
    "    df = pd.DataFrame(X, columns=column_names)\n",
    "    df['T'] = T\n",
    "    df['Y'] = Y\n",
    "\n",
    "\n",
    "    # Convert pandas DataFrame to R DataFrame\n",
    "    r_df = pandas2ri.py2rpy(df)\n",
    "\n",
    "    ro.r.assign(\"p\", p)\n",
    "\n",
    "    # Load the CBPS package in R and fit the model for ATE estimation\n",
    "    ro.r('''\n",
    "        library(CBPS)\n",
    "        estimate_cbps_ate <- function(df) {\n",
    "            formula_str <- paste(\"T ~\", paste(names(df)[1:{p}], collapse=\" + \"))\n",
    "\n",
    "            # Apply CBPS (estimate the ATE, ATT=0)\n",
    "            model <- CBPS(as.formula(formula_str), data = df, ATT = 0, method = \"exact\")\n",
    "\n",
    "            # Retrieve the estimated propensity scores\n",
    "            df$propensity_score <- fitted(model)\n",
    "\n",
    "            # Apply IPW (Inverse Probability Weighting)\n",
    "            df$weight <- ifelse(df$T == 1, 1 / df$propensity_score, 1 / (1 - df$propensity_score))\n",
    "\n",
    "            # Estimate the ATE via weighted regression\n",
    "            result <- lm(Y ~ T, data = df, weights = df$weight)\n",
    "\n",
    "            return(df$propensity_score)\n",
    "        }\n",
    "    ''')\n",
    "\n",
    "    # Call the R function to obtain propensity scores (and the ATE)\n",
    "    est_prop_score = ro.r['estimate_cbps_ate'](r_df)\n",
    "    \n",
    "    est_prop_score_cbps = est_prop_score\n",
    "    \n",
    "    #print(er)\n",
    "\n",
    "    treatment_outcome_model = KernelRegression(kernel=\"rbf\", gamma=np.logspace(-2, 2, 10))\n",
    "    control_outcome_model = KernelRegression(kernel=\"rbf\", gamma=np.logspace(-2, 2, 10))\n",
    "\n",
    "    treatment_outcome_model.fit(X_treatment, Y_treatment)\n",
    "    control_outcome_model.fit(X_control, Y_control)\n",
    "\n",
    "    est_treatment_outcome = treatment_outcome_model.predict(X)\n",
    "    est_control_outcome = control_outcome_model.predict(X)\n",
    "\n",
    "    IPW_est = np.mean(T*Y / est_prop_score - (1 - T)*Y / (1 - est_prop_score))\n",
    "\n",
    "    # Evaluate performance\n",
    "    IPW_bias = IPW_est - true_ATE\n",
    "\n",
    "    print(f\"Estimated ATE: {IPW_est}\")\n",
    "    print(f\"Bias: {IPW_bias}\")\n",
    "    \n",
    "    result_list_temp.append(IPW_est)\n",
    "    \n",
    "    DM_est = np.mean(est_treatment_outcome - est_control_outcome)\n",
    "\n",
    "    # Evaluate performance\n",
    "    DM_bias = DM_est - true_ATE\n",
    "\n",
    "    print(f\"Estimated ATE: {DM_est}\")\n",
    "    print(f\"Bias: {DM_bias}\")\n",
    "    \n",
    "    result_list_temp.append(DM_est)\n",
    "\n",
    "    DR_est = np.mean(T*(Y - est_treatment_outcome) / est_prop_score - (1 - T)*(Y - est_control_outcome)  / (1 - est_prop_score) + est_treatment_outcome - est_control_outcome)\n",
    "\n",
    "    # Evaluate performance\n",
    "    DR_bias = DR_est - true_ATE\n",
    "\n",
    "    print(f\"Estimated ATE: {DR_est}\")\n",
    "    print(f\"Bias: {DR_bias}\")\n",
    "    \n",
    "    result_list_temp.append(DR_est)\n",
    "    \n",
    "    \n",
    "    result_list.append(result_list_temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de51a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a linear model to estimate the treatment effect\n",
    "prop_model = DirectBiasCorrection()\n",
    "prop_model.fit(X, T)\n",
    "est_prop_score = prop_model.predict_proba(X)[:, 1]\n",
    "\n",
    "treatment_outcome_model = KernelRegression(kernel=\"rbf\", gamma=np.logspace(-2, 2, 10))\n",
    "control_outcome_model = KernelRegression(kernel=\"rbf\", gamma=np.logspace(-2, 2, 10))\n",
    "\n",
    "treatment_outcome_model.fit(X_treatment, Y_treatment)\n",
    "control_outcome_model.fit(X_control, Y_control)\n",
    "\n",
    "est_treatment_outcome = treatment_outcome_model.predict(X)\n",
    "est_control_outcome = control_outcome_model.predict(X)\n",
    "\n",
    "IPW_est = np.mean(T*Y / est_prop_score - (1 - T)*Y / (1 - est_prop_score))\n",
    "\n",
    "# Evaluate performance\n",
    "IPW_bias = IPW_est - true_ATE\n",
    "\n",
    "print(f\"Estimated ATE: {IPW_est}\")\n",
    "print(f\"Bias: {IPW_bias}\")\n",
    "\n",
    "DR_est = np.mean(T*(Y - est_treatment_outcome) / est_prop_score - (1 - T)*(Y - est_control_outcome)  / (1 - est_prop_score) + est_treatment_outcome - est_control_outcome)\n",
    "\n",
    "# Evaluate performance\n",
    "DR_bias = DR_est - true_ATE\n",
    "\n",
    "print(f\"Estimated ATE: {DR_est}\")\n",
    "print(f\"Bias: {DR_bias}\")\n",
    "\n",
    "DM_est = np.mean(est_treatment_outcome - est_control_outcome)\n",
    "\n",
    "# Evaluate performance\n",
    "DM_bias = DM_est - true_ATE\n",
    "\n",
    "print(f\"Estimated ATE: {DM_est}\")\n",
    "print(f\"Bias: {DM_bias}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44908346",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects import pandas2ri\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Enable automatic conversion of Pandas DataFrame to R DataFrame\n",
    "\n",
    "# Simulate data in Python\n",
    "\n",
    "# Create a pandas DataFrame\n",
    "column_names = [f'X{i+1}' for i in range(p)]\n",
    "df = pd.DataFrame(X, columns=column_names)\n",
    "df['T'] = T\n",
    "df['Y'] = Y\n",
    "\n",
    "\n",
    "# Convert pandas DataFrame to R DataFrame\n",
    "r_df = pandas2ri.py2rpy(df)\n",
    "\n",
    "ro.r.assign(\"p\", p)\n",
    "\n",
    "# Load the CBPS package in R and fit the model for ATE estimation\n",
    "ro.r('''\n",
    "    library(CBPS)\n",
    "    estimate_cbps_ate <- function(df) {\n",
    "        formula_str <- paste(\"T ~\", paste(names(df)[1:{p}], collapse=\" + \"))\n",
    "        \n",
    "        # Apply CBPS (estimate the ATE, ATT=0)\n",
    "        model <- CBPS(as.formula(formula_str), data = df, ATT = 0, method = \"exact\")\n",
    "        \n",
    "        # Retrieve the estimated propensity scores\n",
    "        df$propensity_score <- fitted(model)\n",
    "        \n",
    "        # Apply IPW (Inverse Probability Weighting)\n",
    "        df$weight <- ifelse(df$T == 1, 1 / df$propensity_score, 1 / (1 - df$propensity_score))\n",
    "        \n",
    "        # Estimate the ATE via weighted regression\n",
    "        result <- lm(Y ~ T, data = df, weights = df$weight)\n",
    "        \n",
    "        return(df$propensity_score)\n",
    "    }\n",
    "''')\n",
    "\n",
    "# Call the R function to obtain propensity scores (and the ATE)\n",
    "est_prop_score = ro.r['estimate_cbps_ate'](r_df)\n",
    "\n",
    "treatment_outcome_model = KernelRegression(kernel=\"rbf\", gamma=np.logspace(-2, 2, 10))\n",
    "control_outcome_model = KernelRegression(kernel=\"rbf\", gamma=np.logspace(-2, 2, 10))\n",
    "\n",
    "treatment_outcome_model.fit(X_treatment, Y_treatment)\n",
    "control_outcome_model.fit(X_control, Y_control)\n",
    "\n",
    "est_treatment_outcome = treatment_outcome_model.predict(X)\n",
    "est_control_outcome = control_outcome_model.predict(X)\n",
    "\n",
    "IPW_est = np.mean(T*Y / est_prop_score - (1 - T)*Y / (1 - est_prop_score))\n",
    "\n",
    "# Evaluate performance\n",
    "IPW_bias = IPW_est - true_ATE\n",
    "\n",
    "print(f\"Estimated ATE: {IPW_est}\")\n",
    "print(f\"Bias: {IPW_bias}\")\n",
    "\n",
    "DR_est = np.mean(T*(Y - est_treatment_outcome) / est_prop_score - (1 - T)*(Y - est_control_outcome)  / (1 - est_prop_score) + est_treatment_outcome - est_control_outcome)\n",
    "\n",
    "# Evaluate performance\n",
    "DR_bias = DR_est - true_ATE\n",
    "\n",
    "print(f\"Estimated ATE: {DR_est}\")\n",
    "print(f\"Bias: {DR_bias}\")\n",
    "\n",
    "DM_est = np.mean(est_treatment_outcome - est_control_outcome)\n",
    "\n",
    "# Evaluate performance\n",
    "DM_bias = DM_est - true_ATE\n",
    "\n",
    "print(f\"Estimated ATE: {DM_est}\")\n",
    "print(f\"Bias: {DM_bias}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4c97e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e10c65b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b576a3f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4262c684",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd21787",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee6de69",
   "metadata": {},
   "outputs": [],
   "source": [
    "est_prop_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce232bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "DM_est = np.mean(est_treatment_outcome - est_control_outcome)\n",
    "\n",
    "# Evaluate performance\n",
    "DM_bias = DM_est - true_ATE\n",
    "\n",
    "print(f\"Estimated ATE: {DM_est}\")\n",
    "print(f\"Bias: {DM_bias}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a04d347",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "IPW_est = np.mean(T*Y / est_prop_score - (1 - T)*Y / (1 - est_prop_score))\n",
    "\n",
    "# Evaluate performance\n",
    "IPW_bias = IPW_est - true_ATE\n",
    "\n",
    "print(f\"Estimated ATE: {IPW_est}\")\n",
    "print(f\"Bias: {IPW_bias}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b04a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ca3335",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d7c72d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5126933",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaa5306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "T_tensor = torch.tensor(T, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Define a simple neural network model for propensity score estimation\n",
    "class PropensityScoreNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(PropensityScoreNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = PropensityScoreNN(p)\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy for binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the neural network\n",
    "num_epochs = 500\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(X_tensor)\n",
    "    loss = criterion(outputs, T_tensor)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Print loss for every 100 epochs\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Estimated propensity scores for all data\n",
    "with torch.no_grad():\n",
    "    estimated_propensity_scores = model(X_tensor).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d649714",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_propensity_scores[estimated_propensity_scores < 0.01] = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0dd494",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_propensity_scores[estimated_propensity_scores > 0.99] = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593f2a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean((T/estimated_propensity_scores.T[0] - (1-T)/(1 - estimated_propensity_scores.T[0]))*Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05ad2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "T_tensor = torch.tensor(T, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Define a simple neural network model for propensity score estimation\n",
    "class PropensityScoreNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(PropensityScoreNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = PropensityScoreNN(p)\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy for binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the neural network\n",
    "num_epochs = 500\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(X_tensor)\n",
    "    outputs = torch.clamp(outputs, min=0.01, max=0.99)\n",
    "    loss = -2*(1/outputs + 1/(1 - outputs)) + (T_tensor / outputs - (1-T_tensor) / (1-outputs))**2\n",
    "    loss = loss.mean()\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Print loss for every 100 epochs\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Estimated propensity scores for all data\n",
    "with torch.no_grad():\n",
    "    estimated_propensity_scores = model(X_tensor).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92534a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_tensor = torch.cat([T_tensor, X_tensor], axis=1)\n",
    "Y_tensor = torch.tensor(Y, dtype=torch.float32).view(-1, 1)\n",
    "dim = Z_tensor.shape[1]\n",
    "\n",
    "class CodOutcomeNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(CodOutcomeNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "# Initialize model, loss function, and optimizer\n",
    "model = CodOutcomeNN(dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Train the neural network\n",
    "num_epochs = 10000\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(Z_tensor)\n",
    "    loss = ((Y_tensor - outputs)**2).mean()\n",
    "    loss = loss.mean()\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Print loss for every 100 epochs\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Estimated propensity scores for all data\n",
    "with torch.no_grad():\n",
    "    estimated_conditional_outcomes = model(Z_tensor).numpy()\n",
    "    estimated_conditional_outcomes_1 = model(Z_tensor_1).numpy()\n",
    "    estimated_conditional_outcomes_0 = model(Z_tensor_0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79088b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    estimated_conditional_outcomes = model(Z_tensor).numpy()\n",
    "    estimated_conditional_outcomes_1 = model(Z_tensor_1).numpy()\n",
    "    estimated_conditional_outcomes_0 = model(Z_tensor_0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b1ac70",
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa = np.mean(estimated_conditional_outcomes_1 - estimated_conditional_outcomes_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7757708",
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8960320f",
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa + np.mean((T/estimated_propensity_scores.T[0] - (1-T)/(1 - estimated_propensity_scores.T[0]))*(Y - estimated_conditional_outcomes.T[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7606f6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y - estimated_conditional_outcomes.T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b606047",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}