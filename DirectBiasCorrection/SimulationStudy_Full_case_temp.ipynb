{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "32cd9a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.special import expit  # Sigmoid function for propensity score\n",
    "from kernel_regression import KernelRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "99e11ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation parameters\n",
    "n = 5000 # Number of samples\n",
    "p = 3    # Number of covariates\n",
    "treatment_effect = 3.0  # True treatment effect\n",
    "\n",
    "# Generate covariates\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62df913a",
   "metadata": {},
   "source": [
    "### Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "681a2551",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.normal(0, 1, (n, p))\n",
    "\n",
    "# Define a propensity score model\n",
    "# Assume treatment probability is a sigmoid function of a subset of covariates\n",
    "propensity_coef = np.random.normal(0, 0.5, p)\n",
    "propensity_scores = expit(X @ propensity_coef)  # Calculate propensity scores\n",
    "\n",
    "# Generate treatment assignment based on propensity scores\n",
    "T = np.random.binomial(1, propensity_scores)\n",
    "\n",
    "# Generate outcome with treatment effect\n",
    "# Assume a simple linear model for demonstration\n",
    "beta = np.random.normal(0, 1, p)\n",
    "Y = (X @ beta)**2 + 1.1 + treatment_effect * T + np.random.normal(0, 1, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "87b0e17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_treatment = X[T == 1]\n",
    "X_control = X[T == 0]\n",
    "\n",
    "Y_treatment = Y[T == 1]\n",
    "Y_control = Y[T == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "1bb0d934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Average Treatment Effect (ATE): 3.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"True Average Treatment Effect (ATE): {true_ATE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "8a9d0835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated ATE: 2.972625870114331\n",
      "Bias: -0.027374129885668896\n",
      "Mean Squared Error: 1.7797254450419215\n"
     ]
    }
   ],
   "source": [
    "# Fit a linear model to estimate the treatment effect\n",
    "model = LinearRegression()\n",
    "model.fit(np.hstack([X, T.reshape(-1, 1)]), Y)\n",
    "estimated_treatment_effect = model.coef_[-1]\n",
    "\n",
    "# Evaluate performance\n",
    "true_ATE = treatment_effect\n",
    "bias = estimated_treatment_effect - true_ATE\n",
    "mse = mean_squared_error(Y, model.predict(np.hstack([X, T.reshape(-1, 1)])))\n",
    "\n",
    "print(f\"Estimated ATE: {estimated_treatment_effect}\")\n",
    "print(f\"Bias: {bias}\")\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "a0f6b42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated ATE: 2.887012893946858\n",
      "Bias: -0.11298710605314222\n",
      "Estimated ATE: 2.9749627896566637\n",
      "Bias: -0.02503721034333628\n",
      "Estimated ATE: 2.97911273489089\n",
      "Bias: -0.020887265109109876\n"
     ]
    }
   ],
   "source": [
    "# Fit a linear model to estimate the treatment effect\n",
    "prop_model = LogisticRegression()\n",
    "prop_model.fit(X, T)\n",
    "est_prop_score = prop_model.predict_proba(X)[:, 1]\n",
    "\n",
    "treatment_outcome_model = KernelRegression(kernel=\"rbf\", gamma=np.logspace(-2, 2, 10))\n",
    "control_outcome_model = KernelRegression(kernel=\"rbf\", gamma=np.logspace(-2, 2, 10))\n",
    "\n",
    "treatment_outcome_model.fit(X_treatment, Y_treatment)\n",
    "control_outcome_model.fit(X_control, Y_control)\n",
    "\n",
    "est_treatment_outcome = treatment_outcome_model.predict(X)\n",
    "est_control_outcome = control_outcome_model.predict(X)\n",
    "\n",
    "IPW_est = np.mean(T*Y / est_prop_score - (1 - T)*Y / (1 - est_prop_score))\n",
    "\n",
    "# Evaluate performance\n",
    "IPW_bias = IPW_est - true_ATE\n",
    "\n",
    "print(f\"Estimated ATE: {IPW_est}\")\n",
    "print(f\"Bias: {IPW_bias}\")\n",
    "\n",
    "DR_est = np.mean(T*(Y - est_treatment_outcome) / est_prop_score - (1 - T)*(Y - est_control_outcome)  / (1 - est_prop_score) + est_treatment_outcome - est_control_outcome)\n",
    "\n",
    "# Evaluate performance\n",
    "DR_bias = DR_est - true_ATE\n",
    "\n",
    "print(f\"Estimated ATE: {DR_est}\")\n",
    "print(f\"Bias: {DR_bias}\")\n",
    "\n",
    "DM_est = np.mean(est_treatment_outcome - est_control_outcome)\n",
    "\n",
    "# Evaluate performance\n",
    "DM_bias = DM_est - true_ATE\n",
    "\n",
    "print(f\"Estimated ATE: {DM_est}\")\n",
    "print(f\"Bias: {DM_bias}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "95e4a042",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "class DirectBiasCorrection:\n",
    "    def __init__(self, initial_theta=None, max_iter=1000, tol=1e-6):\n",
    "        \"\"\"\n",
    "        カスタムロジスティック回帰モデル\n",
    "        :param initial_theta: 初期値 (デフォルトは None でゼロベクトルを使用)\n",
    "        :param max_iter: 最大反復回数\n",
    "        :param tol: 収束許容誤差\n",
    "        \"\"\"\n",
    "        self.initial_theta = initial_theta\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.theta_ = None  # 学習されたパラメータ\n",
    "\n",
    "    def _sigmoid(self, X, theta):\n",
    "        \"\"\"シグモイド関数の定義\"\"\"\n",
    "        return 1 / (1 + np.exp(-np.dot(X, theta)))\n",
    "\n",
    "    def _loss_function(self, theta, X, T):\n",
    "        \"\"\"損失関数の定義\"\"\"\n",
    "        outputs = self._sigmoid(X, theta)\n",
    "        loss = -2 * (1 / outputs + 1 / (1 - outputs)) + (T / outputs - (1 - T) / (1 - outputs)) ** 2\n",
    "        return np.mean(loss)\n",
    "\n",
    "    def fit(self, X, T):\n",
    "        \"\"\"\n",
    "        モデルの学習\n",
    "        :param X: 説明変数（N×d の配列）\n",
    "        :param T: 目的変数（N×1 のバイナリ配列）\n",
    "        \"\"\"\n",
    "        X, T = np.asarray(X), np.asarray(T)\n",
    "\n",
    "        # 初期値の設定（ゼロベクトル）\n",
    "        if self.initial_theta is None:\n",
    "            self.initial_theta = np.zeros(X.shape[1])\n",
    "\n",
    "        # 最適化の実行\n",
    "        result = minimize(\n",
    "            self._loss_function, \n",
    "            self.initial_theta, \n",
    "            args=(X, T), \n",
    "            method='L-BFGS-B',\n",
    "            options={'maxiter': self.max_iter, 'disp': False, 'ftol': self.tol}\n",
    "        )\n",
    "        \n",
    "        self.theta_ = result.x\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        予測確率の計算\n",
    "        :param X: 説明変数（N×d の配列）\n",
    "        :return: クラス1の確率\n",
    "        \"\"\"\n",
    "        if self.theta_ is None:\n",
    "            raise ValueError(\"Model is not fitted yet. Call 'fit' first.\")\n",
    "            \n",
    "        output = self._sigmoid(X, self.theta_)\n",
    "        return np.array([1-output, output]).T\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        クラスラベルの予測\n",
    "        :param X: 説明変数（N×d の配列）\n",
    "        :return: 0または1の予測クラス\n",
    "        \"\"\"\n",
    "        probas = self.predict_proba(X)\n",
    "        return (probas >= 0.5).astype(int)\n",
    "\n",
    "    def get_params(self):\n",
    "        \"\"\"\n",
    "        学習済みパラメータの取得\n",
    "        :return: 推定された θ\n",
    "        \"\"\"\n",
    "        if self.theta_ is None:\n",
    "            raise ValueError(\"Model is not fitted yet. Call 'fit' first.\")\n",
    "        return self.theta_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "7de51a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated ATE: 2.9685072990904686\n",
      "Bias: -0.03149270090953138\n",
      "Estimated ATE: 2.972925625090657\n",
      "Bias: -0.027074374909342946\n",
      "Estimated ATE: 2.97911273489089\n",
      "Bias: -0.020887265109109876\n"
     ]
    }
   ],
   "source": [
    "# Fit a linear model to estimate the treatment effect\n",
    "prop_model = DirectBiasCorrection()\n",
    "prop_model.fit(X, T)\n",
    "est_prop_score = prop_model.predict_proba(X)[:, 1]\n",
    "\n",
    "treatment_outcome_model = KernelRegression(kernel=\"rbf\", gamma=np.logspace(-2, 2, 10))\n",
    "control_outcome_model = KernelRegression(kernel=\"rbf\", gamma=np.logspace(-2, 2, 10))\n",
    "\n",
    "treatment_outcome_model.fit(X_treatment, Y_treatment)\n",
    "control_outcome_model.fit(X_control, Y_control)\n",
    "\n",
    "est_treatment_outcome = treatment_outcome_model.predict(X)\n",
    "est_control_outcome = control_outcome_model.predict(X)\n",
    "\n",
    "IPW_est = np.mean(T*Y / est_prop_score - (1 - T)*Y / (1 - est_prop_score))\n",
    "\n",
    "# Evaluate performance\n",
    "IPW_bias = IPW_est - true_ATE\n",
    "\n",
    "print(f\"Estimated ATE: {IPW_est}\")\n",
    "print(f\"Bias: {IPW_bias}\")\n",
    "\n",
    "DR_est = np.mean(T*(Y - est_treatment_outcome) / est_prop_score - (1 - T)*(Y - est_control_outcome)  / (1 - est_prop_score) + est_treatment_outcome - est_control_outcome)\n",
    "\n",
    "# Evaluate performance\n",
    "DR_bias = DR_est - true_ATE\n",
    "\n",
    "print(f\"Estimated ATE: {DR_est}\")\n",
    "print(f\"Bias: {DR_bias}\")\n",
    "\n",
    "DM_est = np.mean(est_treatment_outcome - est_control_outcome)\n",
    "\n",
    "# Evaluate performance\n",
    "DM_bias = DM_est - true_ATE\n",
    "\n",
    "print(f\"Estimated ATE: {DM_est}\")\n",
    "print(f\"Bias: {DM_bias}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "44908346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated ATE: 2.9577300335945154\n",
      "Bias: -0.04226996640548464\n",
      "Estimated ATE: 2.975807993897503\n",
      "Bias: -0.024192006102496943\n",
      "Estimated ATE: 2.97911273489089\n",
      "Bias: -0.020887265109109876\n"
     ]
    }
   ],
   "source": [
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects import pandas2ri\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Enable automatic conversion of Pandas DataFrame to R DataFrame\n",
    "pandas2ri.activate()\n",
    "\n",
    "# Simulate data in Python\n",
    "\n",
    "# Create a pandas DataFrame\n",
    "column_names = [f'X{i+1}' for i in range(p)]\n",
    "df = pd.DataFrame(X, columns=column_names)\n",
    "df['T'] = T\n",
    "df['Y'] = Y\n",
    "\n",
    "\n",
    "# Convert pandas DataFrame to R DataFrame\n",
    "r_df = pandas2ri.py2rpy(df)\n",
    "\n",
    "ro.r.assign(\"p\", p)\n",
    "\n",
    "# Load the CBPS package in R and fit the model for ATE estimation\n",
    "ro.r('''\n",
    "    library(CBPS)\n",
    "    estimate_cbps_ate <- function(df) {\n",
    "        formula_str <- paste(\"T ~\", paste(names(df)[1:{p}], collapse=\" + \"))\n",
    "        \n",
    "        # CBPSの適用 (ATEの推定、ATT=0)\n",
    "        model <- CBPS(as.formula(formula_str), data = df, ATT = 0, method = \"exact\")\n",
    "        \n",
    "        # 推定された傾向スコアの取得\n",
    "        df$propensity_score <- fitted(model)\n",
    "        \n",
    "        # IPW (Inverse Probability Weighting) を適用\n",
    "        df$weight <- ifelse(df$T == 1, 1 / df$propensity_score, 1 / (1 - df$propensity_score))\n",
    "        \n",
    "        # 重み付き回帰によるATEの推定\n",
    "        result <- lm(Y ~ T, data = df, weights = df$weight)\n",
    "        \n",
    "        return(df$propensity_score)\n",
    "    }\n",
    "''')\n",
    "\n",
    "# R関数を呼び出してATEと傾向スコアを取得\n",
    "est_prop_score = ro.r['estimate_cbps_ate'](r_df)\n",
    "\n",
    "treatment_outcome_model = KernelRegression(kernel=\"rbf\", gamma=np.logspace(-2, 2, 10))\n",
    "control_outcome_model = KernelRegression(kernel=\"rbf\", gamma=np.logspace(-2, 2, 10))\n",
    "\n",
    "treatment_outcome_model.fit(X_treatment, Y_treatment)\n",
    "control_outcome_model.fit(X_control, Y_control)\n",
    "\n",
    "est_treatment_outcome = treatment_outcome_model.predict(X)\n",
    "est_control_outcome = control_outcome_model.predict(X)\n",
    "\n",
    "IPW_est = np.mean(T*Y / est_prop_score - (1 - T)*Y / (1 - est_prop_score))\n",
    "\n",
    "# Evaluate performance\n",
    "IPW_bias = IPW_est - true_ATE\n",
    "\n",
    "print(f\"Estimated ATE: {IPW_est}\")\n",
    "print(f\"Bias: {IPW_bias}\")\n",
    "\n",
    "DR_est = np.mean(T*(Y - est_treatment_outcome) / est_prop_score - (1 - T)*(Y - est_control_outcome)  / (1 - est_prop_score) + est_treatment_outcome - est_control_outcome)\n",
    "\n",
    "# Evaluate performance\n",
    "DR_bias = DR_est - true_ATE\n",
    "\n",
    "print(f\"Estimated ATE: {DR_est}\")\n",
    "print(f\"Bias: {DR_bias}\")\n",
    "\n",
    "DM_est = np.mean(est_treatment_outcome - est_control_outcome)\n",
    "\n",
    "# Evaluate performance\n",
    "DM_bias = DM_est - true_ATE\n",
    "\n",
    "print(f\"Estimated ATE: {DM_est}\")\n",
    "print(f\"Bias: {DM_bias}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4c97e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e10c65b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b576a3f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4262c684",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd21787",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "eee6de69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<rpy2.robjects.functions.SignatureTranslatedFunction object at 0x326c98250> [3]\n",
       "R classes: ('function',)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est_prop_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "bce232bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated ATE: 2.9181776676318285\n",
      "Bias: -0.08182233236817149\n"
     ]
    }
   ],
   "source": [
    "DM_est = np.mean(est_treatment_outcome - est_control_outcome)\n",
    "\n",
    "# Evaluate performance\n",
    "DM_bias = DM_est - true_ATE\n",
    "\n",
    "print(f\"Estimated ATE: {DM_est}\")\n",
    "print(f\"Bias: {DM_bias}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1a04d347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated ATE: 3.067399653974786\n",
      "Bias: 0.06739965397478587\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "IPW_est = np.mean(T*Y / est_prop_score - (1 - T)*Y / (1 - est_prop_score))\n",
    "\n",
    "# Evaluate performance\n",
    "IPW_bias = IPW_est - true_ATE\n",
    "\n",
    "print(f\"Estimated ATE: {IPW_est}\")\n",
    "print(f\"Bias: {IPW_bias}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "39b04a76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,\n",
       "       1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1,\n",
       "       1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,\n",
       "       0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,\n",
       "       1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0,\n",
       "       0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,\n",
       "       1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1,\n",
       "       0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,\n",
       "       0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,\n",
       "       0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "       0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,\n",
       "       0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1,\n",
       "       0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "       0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,\n",
       "       0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,\n",
       "       1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,\n",
       "       0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0,\n",
       "       1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "       0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1,\n",
       "       1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0,\n",
       "       0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "       0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,\n",
       "       0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,\n",
       "       0, 0, 1, 0, 1, 0, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ca3335",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d7c72d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e5126933",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaa5306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "T_tensor = torch.tensor(T, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Define a simple neural network model for propensity score estimation\n",
    "class PropensityScoreNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(PropensityScoreNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = PropensityScoreNN(p)\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy for binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the neural network\n",
    "num_epochs = 500\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(X_tensor)\n",
    "    loss = criterion(outputs, T_tensor)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Print loss for every 100 epochs\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Estimated propensity scores for all data\n",
    "with torch.no_grad():\n",
    "    estimated_propensity_scores = model(X_tensor).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "0d649714",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_propensity_scores[estimated_propensity_scores < 0.01] = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "fb0dd494",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_propensity_scores[estimated_propensity_scores > 0.99] = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "593f2a03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.011218015922772"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((T/estimated_propensity_scores.T[0] - (1-T)/(1 - estimated_propensity_scores.T[0]))*Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "a05ad2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/500], Loss: -89.3757\n",
      "Epoch [200/500], Loss: -77.3542\n",
      "Epoch [300/500], Loss: -126.3577\n",
      "Epoch [400/500], Loss: -137.7103\n",
      "Epoch [500/500], Loss: -146.9600\n"
     ]
    }
   ],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "T_tensor = torch.tensor(T, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Define a simple neural network model for propensity score estimation\n",
    "class PropensityScoreNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(PropensityScoreNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = PropensityScoreNN(p)\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy for binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the neural network\n",
    "num_epochs = 500\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(X_tensor)\n",
    "    outputs = torch.clamp(outputs, min=0.01, max=0.99)\n",
    "    loss = -2*(1/outputs + 1/(1 - outputs)) + (T_tensor / outputs - (1-T_tensor) / (1-outputs))**2\n",
    "    loss = loss.mean()\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Print loss for every 100 epochs\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Estimated propensity scores for all data\n",
    "with torch.no_grad():\n",
    "    estimated_propensity_scores = model(X_tensor).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "92534a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/10000], Loss: 66.5176\n",
      "Epoch [200/10000], Loss: 55.3388\n",
      "Epoch [300/10000], Loss: 43.7221\n",
      "Epoch [400/10000], Loss: 34.2139\n",
      "Epoch [500/10000], Loss: 28.5578\n",
      "Epoch [600/10000], Loss: 25.6987\n",
      "Epoch [700/10000], Loss: 23.8395\n",
      "Epoch [800/10000], Loss: 22.1396\n",
      "Epoch [900/10000], Loss: 20.4078\n",
      "Epoch [1000/10000], Loss: 18.6166\n",
      "Epoch [1100/10000], Loss: 16.7452\n",
      "Epoch [1200/10000], Loss: 14.7996\n",
      "Epoch [1300/10000], Loss: 12.8643\n",
      "Epoch [1400/10000], Loss: 10.9603\n",
      "Epoch [1500/10000], Loss: 9.1128\n",
      "Epoch [1600/10000], Loss: 7.3873\n",
      "Epoch [1700/10000], Loss: 5.9171\n",
      "Epoch [1800/10000], Loss: 4.7640\n",
      "Epoch [1900/10000], Loss: 3.9100\n",
      "Epoch [2000/10000], Loss: 3.3287\n",
      "Epoch [2100/10000], Loss: 2.9629\n",
      "Epoch [2200/10000], Loss: 2.7293\n",
      "Epoch [2300/10000], Loss: 2.5687\n",
      "Epoch [2400/10000], Loss: 2.4482\n",
      "Epoch [2500/10000], Loss: 2.3496\n",
      "Epoch [2600/10000], Loss: 2.2630\n",
      "Epoch [2700/10000], Loss: 2.1825\n",
      "Epoch [2800/10000], Loss: 2.1069\n",
      "Epoch [2900/10000], Loss: 2.0346\n",
      "Epoch [3000/10000], Loss: 1.9642\n",
      "Epoch [3100/10000], Loss: 1.8966\n",
      "Epoch [3200/10000], Loss: 1.8318\n",
      "Epoch [3300/10000], Loss: 1.7691\n",
      "Epoch [3400/10000], Loss: 1.7078\n",
      "Epoch [3500/10000], Loss: 1.6470\n",
      "Epoch [3600/10000], Loss: 1.5847\n",
      "Epoch [3700/10000], Loss: 1.5225\n",
      "Epoch [3800/10000], Loss: 1.4613\n",
      "Epoch [3900/10000], Loss: 1.4030\n",
      "Epoch [4000/10000], Loss: 1.3455\n",
      "Epoch [4100/10000], Loss: 1.2895\n",
      "Epoch [4200/10000], Loss: 1.2348\n",
      "Epoch [4300/10000], Loss: 1.1828\n",
      "Epoch [4400/10000], Loss: 1.1329\n",
      "Epoch [4500/10000], Loss: 1.0857\n",
      "Epoch [4600/10000], Loss: 1.0390\n",
      "Epoch [4700/10000], Loss: 0.9945\n",
      "Epoch [4800/10000], Loss: 0.9516\n",
      "Epoch [4900/10000], Loss: 0.9076\n",
      "Epoch [5000/10000], Loss: 0.8658\n",
      "Epoch [5100/10000], Loss: 0.8268\n",
      "Epoch [5200/10000], Loss: 0.7897\n",
      "Epoch [5300/10000], Loss: 0.7530\n",
      "Epoch [5400/10000], Loss: 0.7188\n",
      "Epoch [5500/10000], Loss: 0.6856\n",
      "Epoch [5600/10000], Loss: 0.6538\n",
      "Epoch [5700/10000], Loss: 0.6236\n",
      "Epoch [5800/10000], Loss: 0.5945\n",
      "Epoch [5900/10000], Loss: 0.5659\n",
      "Epoch [6000/10000], Loss: 0.5379\n",
      "Epoch [6100/10000], Loss: 0.5098\n",
      "Epoch [6200/10000], Loss: 0.4821\n",
      "Epoch [6300/10000], Loss: 0.4556\n",
      "Epoch [6400/10000], Loss: 0.4308\n",
      "Epoch [6500/10000], Loss: 0.4067\n",
      "Epoch [6600/10000], Loss: 0.3829\n",
      "Epoch [6700/10000], Loss: 0.3603\n",
      "Epoch [6800/10000], Loss: 0.3383\n",
      "Epoch [6900/10000], Loss: 0.3168\n",
      "Epoch [7000/10000], Loss: 0.2955\n",
      "Epoch [7100/10000], Loss: 0.2755\n",
      "Epoch [7200/10000], Loss: 0.2567\n",
      "Epoch [7300/10000], Loss: 0.2394\n",
      "Epoch [7400/10000], Loss: 0.2231\n",
      "Epoch [7500/10000], Loss: 0.2078\n",
      "Epoch [7600/10000], Loss: 0.1936\n",
      "Epoch [7700/10000], Loss: 0.1792\n",
      "Epoch [7800/10000], Loss: 0.1660\n",
      "Epoch [7900/10000], Loss: 0.1534\n",
      "Epoch [8000/10000], Loss: 0.1417\n",
      "Epoch [8100/10000], Loss: 0.1305\n",
      "Epoch [8200/10000], Loss: 0.1198\n",
      "Epoch [8300/10000], Loss: 0.1089\n",
      "Epoch [8400/10000], Loss: 0.0990\n",
      "Epoch [8500/10000], Loss: 0.0899\n",
      "Epoch [8600/10000], Loss: 0.0819\n",
      "Epoch [8700/10000], Loss: 0.0740\n",
      "Epoch [8800/10000], Loss: 0.0669\n",
      "Epoch [8900/10000], Loss: 0.0607\n",
      "Epoch [9000/10000], Loss: 0.0550\n",
      "Epoch [9100/10000], Loss: 0.0497\n",
      "Epoch [9200/10000], Loss: 0.0449\n",
      "Epoch [9300/10000], Loss: 0.0405\n",
      "Epoch [9400/10000], Loss: 0.0366\n",
      "Epoch [9500/10000], Loss: 0.0330\n",
      "Epoch [9600/10000], Loss: 0.0295\n",
      "Epoch [9700/10000], Loss: 0.0261\n",
      "Epoch [9800/10000], Loss: 0.0230\n",
      "Epoch [9900/10000], Loss: 0.0202\n",
      "Epoch [10000/10000], Loss: 0.0178\n"
     ]
    }
   ],
   "source": [
    "Z_tensor = torch.cat([T_tensor, X_tensor], axis=1)\n",
    "Y_tensor = torch.tensor(Y, dtype=torch.float32).view(-1, 1)\n",
    "dim = Z_tensor.shape[1]\n",
    "\n",
    "class CodOutcomeNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(CodOutcomeNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "# Initialize model, loss function, and optimizer\n",
    "model = CodOutcomeNN(dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Train the neural network\n",
    "num_epochs = 10000\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(Z_tensor)\n",
    "    loss = ((Y_tensor - outputs)**2).mean()\n",
    "    loss = loss.mean()\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Print loss for every 100 epochs\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Estimated propensity scores for all data\n",
    "with torch.no_grad():\n",
    "    estimated_conditional_outcomes = model(Z_tensor).numpy()\n",
    "    estimated_conditional_outcomes_1 = model(Z_tensor_1).numpy()\n",
    "    estimated_conditional_outcomes_0 = model(Z_tensor_0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "79088b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    estimated_conditional_outcomes = model(Z_tensor).numpy()\n",
    "    estimated_conditional_outcomes_1 = model(Z_tensor_1).numpy()\n",
    "    estimated_conditional_outcomes_0 = model(Z_tensor_0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "78b1ac70",
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa = np.mean(estimated_conditional_outcomes_1 - estimated_conditional_outcomes_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "a7757708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.98163"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "8960320f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.979416648378537"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aaa + np.mean((T/estimated_propensity_scores.T[0] - (1-T)/(1 - estimated_propensity_scores.T[0]))*(Y - estimated_conditional_outcomes.T[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "7606f6ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.63514307e-01,  4.77596318e-01,  6.29739734e-02, -1.27126940e-01,\n",
       "       -1.77228238e-01,  4.64099619e-01, -3.51511405e-01, -2.99983419e-01,\n",
       "       -2.69382789e-01,  1.26012672e-01, -3.40216497e-03,  2.07889849e-01,\n",
       "       -6.60575762e-01, -4.97077542e-01, -8.41113774e-01, -1.26968687e-01,\n",
       "       -4.25912768e-01, -9.33943451e-02, -4.43000561e-01,  2.44456645e-01,\n",
       "        7.93688016e-02, -7.86699426e-01,  6.66016525e-01,  5.07777333e-01,\n",
       "       -6.60204782e-01,  1.76161963e-01,  3.64499704e-01, -2.11489539e-01,\n",
       "       -3.55552370e-01, -6.61527685e-01, -7.72481761e-02, -2.88697698e-01,\n",
       "       -6.51645266e-01, -1.14941385e+00, -5.81560010e-01,  9.66436085e-01,\n",
       "       -7.85939945e-01, -4.66742301e-01,  7.49621729e-01,  2.88066177e-01,\n",
       "       -5.62568961e-01, -1.75046911e-01,  2.87174176e-01,  5.13794861e-01,\n",
       "       -3.57863637e-01, -6.20851911e-01, -4.15798049e-01, -4.12468909e-01,\n",
       "        2.46813750e-01, -5.61748112e-01,  7.15916546e-01, -6.73407462e-01,\n",
       "       -1.76732254e-01, -2.40935206e+00, -1.59992345e-01, -5.76852512e-01,\n",
       "        1.49646012e-03,  8.09367468e-01, -7.94817237e-01, -1.32114606e+00,\n",
       "       -6.36160314e-01,  1.42659490e-01,  8.44308008e-01, -5.11885701e-01,\n",
       "        8.28024091e-01,  2.59662831e-01,  1.00140264e+00, -1.92462394e-01,\n",
       "        7.87414484e-01,  6.32030264e-01, -1.19802339e+00, -8.09541043e-01,\n",
       "       -5.87295543e-01, -1.13594511e+00, -8.86586069e-02, -3.30804750e-02,\n",
       "        5.21100651e-02,  4.10478994e-01, -2.67739222e-01, -3.73656702e-01,\n",
       "        9.37607527e-01,  6.05151753e-01,  5.63680403e-01,  3.00808513e-01,\n",
       "       -3.37178344e-01,  4.36408132e-01, -4.19100154e-01, -3.99888703e-01,\n",
       "       -7.03016578e-01, -7.28173441e-01,  6.11577219e-01,  4.64185163e-02,\n",
       "       -1.03403020e+00, -1.30089608e-01, -7.69167621e-01,  6.14545964e-01,\n",
       "       -1.91780562e-01, -1.06568119e+00, -7.12366691e-01,  3.06580169e-01,\n",
       "       -8.56431571e-02, -1.39833219e-01,  2.74402072e-02, -3.35936074e-01,\n",
       "       -4.46027341e-01,  4.74311351e-01,  4.12633839e+00, -9.24870624e-02,\n",
       "       -6.33818200e-03, -1.91320617e-01, -4.87347074e-01,  3.22560695e-01,\n",
       "        1.21260588e+00, -5.22162173e-01,  1.36318051e+00,  1.27780740e+00,\n",
       "       -4.00752496e-01, -2.60781920e-01, -8.17196544e-01,  1.71614045e-01,\n",
       "       -6.10198951e-01,  2.38336753e-02, -2.56730357e-01,  7.36549234e-01,\n",
       "       -3.82952376e-01, -1.36913443e+00,  4.52804124e-01, -1.84866249e+00,\n",
       "        4.37583370e-01, -2.08882101e+00, -1.48415051e+00,  6.79362450e-01,\n",
       "       -6.89855678e-01, -7.06069187e-01,  2.58385222e-01, -4.96007550e-01,\n",
       "        1.38506240e+00,  5.53685505e-01, -7.49690221e-01,  6.58784938e-01,\n",
       "       -7.36675269e-01,  1.95396674e+00, -7.60751595e-01,  9.20066715e-01,\n",
       "       -1.33417993e-01,  9.99606271e-01, -6.76830985e-01,  1.38742062e-01,\n",
       "       -1.05293804e+00,  5.21137163e-01,  1.71677823e-01, -5.09288866e-01,\n",
       "       -9.54212482e-01,  4.26331463e-03, -5.88345413e-01, -5.62780320e-01,\n",
       "        2.80358915e-01, -1.12218556e+00,  3.14250817e-01,  4.54526267e-01,\n",
       "        2.89030948e-01, -5.73004326e-01, -2.85095642e-01, -1.89332287e-01,\n",
       "        9.99845902e-01,  4.47829581e-01, -2.57353804e-01,  3.66597768e-01,\n",
       "        5.29472775e-02, -9.64793723e-01,  1.16383972e+00,  7.38348861e-01,\n",
       "       -9.35523716e-01,  4.08139776e-01, -3.23762995e-02, -3.51564046e-02,\n",
       "        6.70266570e-01,  5.19435857e-01,  1.03300505e+00, -8.38275423e-02,\n",
       "        1.51614410e-01, -2.98872333e-01,  3.05934149e-01,  1.36578843e+00,\n",
       "       -5.72184948e-01, -1.56072792e+00, -6.22887239e-01,  2.13789784e-01,\n",
       "        8.29070414e-01,  1.00266472e+00,  3.69073719e-01,  2.93908172e-01,\n",
       "       -5.88880544e-01,  3.12960223e-01,  8.65348420e-01,  1.57438047e-01,\n",
       "        6.94214052e-01,  2.39567815e-02, -7.34875760e-01, -4.76509820e-01,\n",
       "       -2.18185820e-01,  1.94512020e-01,  4.96227467e-01, -1.72539079e-01,\n",
       "       -4.30026474e-01, -6.41232298e-01,  2.20329943e-02,  6.76656016e-01,\n",
       "       -5.59151970e-01, -2.69444798e-01, -1.08250824e+00,  5.75301173e-01,\n",
       "        6.90679545e-01, -1.28882239e+00,  3.53526078e-02,  4.22465502e-01,\n",
       "        7.84558078e-01,  7.38144585e-01, -2.21142934e-01, -9.34604902e-01,\n",
       "       -3.70138266e-01,  2.16080024e-01, -6.39978298e-01, -5.16470941e-01,\n",
       "       -8.08767058e-01,  5.55612692e-01, -1.00452434e+00, -1.21204617e+00,\n",
       "        5.73930309e-01,  9.90717820e-02,  5.90877993e-02, -2.78868283e+00,\n",
       "       -2.89510102e-01, -6.19157148e-01, -7.96334295e-02,  7.26026649e-01,\n",
       "        6.22802169e-01,  7.70152645e-01,  5.64471146e-01,  1.48760792e-01,\n",
       "        3.53825620e-01,  1.84055368e+00,  8.92549927e-01, -1.42841894e+00,\n",
       "       -1.51281439e-01, -1.38422303e-01,  9.73021574e-01, -2.71480932e-01,\n",
       "       -6.93726991e-01, -7.26839438e-01, -7.13848952e-01,  1.16658188e+00,\n",
       "       -2.22805146e-01, -4.49689536e-01, -7.90424724e-01,  1.85254136e+00,\n",
       "        1.67795401e+00, -8.84431226e-01,  1.66361090e-01,  3.59371288e-01,\n",
       "        7.41522324e-01, -1.48645767e-03, -6.84571270e-01, -1.06436141e+00,\n",
       "       -2.41629421e-01, -3.93200658e-01, -6.73377322e-01,  4.95774815e-01,\n",
       "       -1.63588688e-01, -4.68610951e-01, -5.79720027e-01,  5.69575679e-01,\n",
       "        3.85430871e-02,  8.08568303e-01, -8.86018095e-02, -3.07806799e-01,\n",
       "        2.32349029e-01,  3.18222920e-01,  4.23017125e-01,  1.17073515e-01,\n",
       "       -1.49955544e+00,  1.62473412e+00,  8.07164230e-01,  6.68617053e-01,\n",
       "       -3.45061118e-01,  2.58590038e-01, -8.42394414e-01,  7.86752564e-01,\n",
       "       -1.13725100e+00,  1.14786436e+00,  5.96673582e-01, -3.19394242e-01,\n",
       "        8.29688624e-01, -6.58874419e-01, -5.38141257e-01, -1.63007462e-01,\n",
       "        3.18385524e-01, -1.14260964e+00,  7.54047058e-02, -6.53989344e-01,\n",
       "       -2.10119613e-01, -5.99578813e-01, -6.14126053e-01,  5.40936626e-01,\n",
       "        2.65809709e-01,  2.29582658e-01,  1.25324193e-01, -9.03954836e-03,\n",
       "        2.41314736e-01, -5.83291938e-01, -3.56929756e-02,  8.42364721e-01,\n",
       "       -1.30258356e+00,  2.28525584e-01,  8.37208971e-01,  4.37320973e-01,\n",
       "        7.34529858e-01,  3.36348302e-01, -2.02523633e-01, -7.40147906e-01,\n",
       "       -1.44778374e-01,  6.76738674e-01, -1.02363174e+00,  5.34015439e-01,\n",
       "        1.09198616e+00,  1.10112820e+00, -4.65379263e-01, -8.79720810e-01,\n",
       "       -2.52428242e-01, -2.29021724e-01, -5.80402099e-01,  1.71582095e-01,\n",
       "       -3.69449494e-01,  7.73014195e-01, -4.97484391e-01,  8.61949344e-01,\n",
       "        3.21996979e-01, -2.97177233e-01, -4.00374237e-01,  3.19321218e-01,\n",
       "       -9.52838684e-01,  6.04634249e-01,  7.41376406e-01,  8.50683379e-01,\n",
       "       -7.44355725e-01, -1.14753657e+00, -2.01552144e-01,  1.30280187e+00,\n",
       "        1.87290180e-01, -2.96205833e-01,  3.95667840e-02,  2.03474862e+00,\n",
       "        2.71491504e-01, -4.08054665e-01, -1.34561973e+00,  1.26871363e+00,\n",
       "        1.46417008e+00,  3.98786069e-01, -4.01056639e-01,  6.70316088e-01,\n",
       "       -4.65997003e-01, -5.84172893e-01, -9.00766982e-02,  4.81216699e-01,\n",
       "       -1.36632288e+00,  5.04055189e-01,  3.74985868e-01,  1.98258943e-01,\n",
       "        7.56754323e-01,  4.65264375e-01, -4.86823450e-01, -6.93054161e-01,\n",
       "       -1.12495493e+00,  3.87807831e-01,  6.16588836e-01, -7.84758297e-03,\n",
       "        4.13463604e-01, -2.75052787e-01,  3.06112437e-01,  3.52593987e-01,\n",
       "       -2.05507283e-01, -1.04962655e+00,  2.64134122e-01, -5.87247247e-01,\n",
       "       -2.52864126e-01, -3.19387022e-01, -2.06138449e-02,  9.00031151e-01,\n",
       "       -8.10955306e-01,  1.17387481e+00,  7.73949162e-01,  5.05847905e-01,\n",
       "        1.68101279e-01, -1.68550905e+00,  5.91550784e-01,  1.07469156e-01,\n",
       "        9.42384314e-01, -3.95200494e-01,  2.42403571e-01,  1.67227743e-01,\n",
       "        8.02757009e-01,  9.10838928e-01,  2.61392744e-01, -2.16631602e-01,\n",
       "        1.04060685e-01,  1.33108327e-03,  8.36846812e-02,  1.25060532e+00,\n",
       "       -1.08902534e-01, -5.06544567e-01,  9.68225576e-02, -1.68837653e-01,\n",
       "       -4.41222151e-01, -4.36421600e-01,  2.26536836e-01, -1.41832286e+00,\n",
       "        1.32562559e+00, -1.53293908e+00,  2.74797037e+00, -1.89107769e-01,\n",
       "        8.91699135e-02,  2.61444404e-02, -1.87496018e-01, -1.10416006e-01,\n",
       "        1.87810118e-01, -2.92998734e-01,  9.81330381e-03,  4.42285327e-01,\n",
       "       -1.16430778e+00,  5.22160827e-01, -1.92617269e-01, -1.55905039e+00,\n",
       "       -9.76050573e-01,  9.24906748e-01, -3.14754224e-01,  4.05471301e-01,\n",
       "        9.46917322e-01, -9.65207281e-01,  2.15405117e-01,  4.18304390e-01,\n",
       "        6.16448121e-01,  6.47573016e-01,  8.30473109e-01,  4.33756326e-01,\n",
       "       -2.36681805e-01, -1.86736319e-01,  2.50028167e-01,  8.14278137e-01,\n",
       "       -1.60805916e+00, -2.87947800e-01, -5.57956156e-01, -7.04939634e-01,\n",
       "       -3.11152067e-01, -5.66357778e-01, -2.74097138e-01, -8.79507853e-01,\n",
       "       -1.09898839e+00, -9.19927231e-02,  1.37901863e-02,  6.24032656e-01,\n",
       "        8.27014405e-01,  1.16121318e-01,  6.38443288e-01, -3.92711800e-01,\n",
       "       -5.08986098e-02,  6.16851847e-01,  5.22167506e-01, -1.38717917e-01,\n",
       "        1.30795350e+00,  4.32435253e-02,  3.16145260e-01,  3.06508706e-03,\n",
       "        8.04767816e-01, -1.50618153e-01,  1.04339049e-01, -8.32993494e-02,\n",
       "       -8.95986794e-02, -4.92991769e-01, -3.32785209e-01, -2.14104620e-01,\n",
       "       -1.80334507e-01,  1.21197270e+00,  5.91561397e-01,  4.56877608e-01,\n",
       "        2.08195366e-01,  6.54679029e-01, -1.68698954e+00, -3.10047425e-01,\n",
       "       -5.26310372e-01,  3.98282298e-03,  1.02089092e-01, -1.84075766e-01,\n",
       "        6.46755780e-01,  3.88264698e-01, -3.79731580e-01, -1.39180947e+00,\n",
       "       -2.46540441e-01, -2.45730409e-01, -3.63293299e-01, -5.34589390e-01,\n",
       "       -8.45071064e-01, -1.03354332e+00, -2.61820078e-02, -9.20347135e-02,\n",
       "       -1.21415896e+00, -8.32844943e-01, -4.91401911e-01, -8.63429492e-01,\n",
       "        4.30308925e-01, -2.57388376e-01, -1.30870793e-01,  2.82907254e-01,\n",
       "        3.40617438e-01,  1.12653465e+00, -2.24153547e-01,  3.45383388e-01,\n",
       "        3.05935511e-01,  5.78757241e-01,  1.77982318e-01,  6.37763795e-01,\n",
       "       -9.49495648e-02, -4.46896510e-01, -2.54580738e-01,  2.14153722e-01,\n",
       "       -1.80235578e-01,  4.48414391e-01, -3.13460840e-01,  8.04635962e-02,\n",
       "        1.20333680e-01, -6.77313501e-01,  1.62573997e+00,  1.80898061e-02,\n",
       "       -6.82712569e-01, -4.68076738e-01, -2.50670113e-01,  2.77041357e-02,\n",
       "        1.09696019e-03, -1.63756417e-01, -5.04240162e-01,  6.88380323e-01,\n",
       "       -1.01204535e-01, -2.62743244e-01, -8.13019291e-01,  5.55416065e-01,\n",
       "        8.08862783e-01,  7.42397493e-01,  5.51148280e-01,  9.07273521e-01,\n",
       "       -1.88155406e-01,  3.15872141e-01, -1.02428136e-01,  3.32332398e-01,\n",
       "        1.10101112e+00, -4.65087543e-01, -2.28316112e-01, -1.78229352e-01,\n",
       "        3.22479860e-01,  5.71255958e-01, -2.74325630e-01,  1.05809567e+00,\n",
       "       -1.08374322e+00,  9.47573177e-01, -2.09749100e-01, -1.99849206e+00,\n",
       "       -1.62501649e-01,  1.26454881e-01, -6.16892090e-01, -5.78223478e-01,\n",
       "       -1.46244249e-01,  5.79469532e-01,  4.71156291e-01,  9.99529794e-02,\n",
       "        7.56797121e-01, -2.28462948e-02, -1.77070356e+00, -1.15437918e+00,\n",
       "        5.56099141e-01, -3.91791152e-01, -9.91413414e-02,  2.11694004e-01,\n",
       "        6.08917003e-01, -6.03080017e-01, -6.13487864e-01,  3.86641757e-01,\n",
       "       -4.22854721e-01, -1.07845843e+00, -1.09680237e-01, -3.02888630e-01,\n",
       "       -5.35050828e-01, -6.19918589e-01,  2.00972174e-02,  1.33234623e-01,\n",
       "        1.89872995e-01,  7.11402635e-02, -2.07936124e-01, -1.09040064e+00,\n",
       "        7.39643429e-01, -4.28661747e-01, -3.19255304e-01, -7.51637888e-02,\n",
       "       -4.90077450e-01,  1.40980899e+00, -3.60117729e-03, -9.93091725e-01,\n",
       "       -5.79907890e-01, -2.66982203e-01,  1.85597794e-01,  9.89452143e-02,\n",
       "       -4.75947815e-01, -4.22224459e-02, -1.02297732e-01, -7.66900440e-02,\n",
       "       -1.46886229e-01,  6.50776846e-01, -2.41504261e-01,  3.35148972e-02,\n",
       "       -1.71248545e+00, -1.56143790e-01, -1.77002414e+00,  1.54324285e-01,\n",
       "       -4.95407414e-01,  9.14195222e-01, -2.92545432e-01, -8.75828247e-02,\n",
       "       -1.63777875e+00,  6.16666931e-01, -1.99171484e+00, -3.96294904e-01,\n",
       "        1.07228594e-01,  1.04493212e+00, -3.07077028e-01,  1.02843358e-01,\n",
       "       -2.85998142e-01, -8.55982646e-01, -1.49803702e-01, -2.97825079e-01,\n",
       "       -1.64483104e-01,  5.15169594e-02, -7.89832404e-01,  5.78229657e-01,\n",
       "        5.77081529e-01, -4.34013659e-01, -9.66022012e-01, -4.69948382e-03,\n",
       "       -1.51792608e+00, -1.24078237e+00,  1.77795144e+00,  5.77199674e-01,\n",
       "        8.07748378e-01,  1.29929672e+00, -3.39939636e-01, -4.00217505e-01,\n",
       "        4.75307108e-01,  1.45269781e-01, -5.99652663e-01, -1.06792624e+00,\n",
       "        2.00904734e+00, -5.80992839e-01,  2.46485017e-01, -6.64860904e-02,\n",
       "       -6.68233076e-01, -4.66919562e-01,  8.39650586e-01,  2.83296626e-02,\n",
       "       -6.63009508e-01,  1.96263264e-01, -1.43806172e+00,  6.66950023e-01,\n",
       "        1.65536270e-01,  3.18975904e-01, -2.32852196e-01, -3.23043871e-01,\n",
       "        2.54107454e-01, -4.97294547e-02, -1.55696852e-01, -5.58115714e-01,\n",
       "       -8.69246627e-01, -7.20282723e-01, -1.27459453e+00, -6.18228594e-02,\n",
       "       -1.80743143e-01,  5.56777205e-01,  2.44291265e-01,  1.49211271e-02,\n",
       "       -4.47571356e-01, -1.82976221e-02,  7.10144685e-01,  4.13129784e-01,\n",
       "        1.29905242e-02,  6.45871268e-01, -1.70521118e-01, -4.21464858e-01,\n",
       "        7.10836204e-01,  1.61830688e-01,  9.72562867e-01,  2.93698842e-02,\n",
       "        1.28711164e+00, -5.92049692e-02,  5.44929008e-01,  6.26210680e-02,\n",
       "        3.92243835e-01,  1.86549583e-01,  1.02748179e+00, -2.82623492e-01,\n",
       "       -6.41282516e-01, -9.87367356e-01, -1.77780122e-01,  1.52985348e+00,\n",
       "        5.84234020e-01,  4.25256098e+00,  2.18257439e-01,  5.71995813e-01,\n",
       "        2.69897309e-01, -1.85761168e+00,  2.20619607e-01, -1.75743653e+00,\n",
       "       -7.25553581e-01,  8.75056351e-01, -1.20740088e-01, -1.21977928e-01,\n",
       "        3.97828681e-01, -3.78638760e-02, -2.73106501e-01, -3.93057829e-01,\n",
       "       -3.29740402e-01, -5.44138855e-01,  9.97567245e-01,  9.46565539e-01,\n",
       "        7.47011190e-01,  3.40859581e-01, -3.61523071e-02,  1.01204519e+00,\n",
       "       -3.87646595e-01,  6.06760074e-01,  3.51144164e-01,  1.24844002e+00,\n",
       "        7.81251018e-01,  3.43605267e-01, -4.49286095e-01, -1.24168019e+00,\n",
       "        1.01983018e-01, -8.26989711e-01,  3.62726358e-01, -6.62764730e-01,\n",
       "        1.07045400e+00,  1.23305883e+00,  1.98655978e-01, -5.74507223e-02,\n",
       "        8.21161591e-01, -1.26003147e+00,  1.48731975e-01,  2.43676176e-01,\n",
       "        3.01695529e-01, -6.07322211e-01, -1.75048334e-01,  1.82937674e-01,\n",
       "        2.98167140e-02, -3.61597897e-01,  3.64414699e-01,  4.71883715e-01,\n",
       "       -5.88439397e-02, -1.25828197e-01,  4.79954567e-01,  3.74345023e-01,\n",
       "       -4.65230760e-01, -2.42180422e-01, -4.36458479e-02,  2.18234474e-01,\n",
       "       -4.78110734e-01, -7.82191857e-02,  1.92438912e+00, -6.14304824e-01,\n",
       "       -6.59973673e-01,  3.08973833e-01,  6.34919938e-02,  2.17469319e-01,\n",
       "        1.56342365e+00,  1.00105464e+00, -9.94328965e-01, -3.87521327e-01,\n",
       "        3.85908172e-01, -1.07898214e+00,  8.90814901e-01, -5.47852178e-01,\n",
       "        1.10818156e+00,  6.44776180e-01,  3.64542766e-01,  4.56011348e-02,\n",
       "       -4.33988996e-01,  2.16666607e+00, -6.10610098e-01,  4.25140676e-03,\n",
       "       -1.07060133e+00, -4.74806455e-01,  1.94293081e-03, -2.34212121e-01,\n",
       "       -1.32856648e+00, -6.48400236e-01, -4.21740275e-01,  1.21265599e-01,\n",
       "       -1.32052527e+00, -7.98744295e-01, -3.49558025e-01,  8.21903716e-01,\n",
       "        8.01906857e-01, -5.80867747e-01,  5.13901006e-01,  3.81784061e-01,\n",
       "       -8.85999377e-01,  7.33845897e-01,  7.04706942e-01,  5.96683179e-01,\n",
       "       -3.71046785e-01,  2.82093651e-01, -1.70522657e-01, -4.70371872e-01,\n",
       "        4.78429548e-01, -8.62091076e-01, -4.00225057e-01, -6.04110115e-01,\n",
       "        6.92473644e-01,  3.76542879e-01,  8.15911307e-01, -2.43059168e-01,\n",
       "        9.47393482e-01,  1.97308046e-01, -2.06426140e-01, -8.52063884e-01,\n",
       "        9.00692285e-01,  1.28538907e+00,  3.07233351e-01,  2.31658008e-01,\n",
       "       -4.70594474e-01, -6.94490466e-02,  7.79284127e-01, -1.58148201e+00,\n",
       "        6.23142517e-01,  6.76888234e-01,  6.72298592e-01, -1.44102717e-01,\n",
       "        5.11153196e-01,  1.04373783e+00,  9.15016558e-01, -1.07343741e-01,\n",
       "       -2.78384061e-01,  3.88160055e-01, -2.03111464e+00, -5.89094173e-01,\n",
       "        6.78726584e-01,  9.68658832e-01,  1.65425256e-01, -1.63138557e-01,\n",
       "       -5.67964663e-01,  5.69397620e-01,  1.41163807e+00, -2.54718356e-01,\n",
       "       -9.43023875e-02, -4.63603660e-01,  3.27720240e-01, -1.21152451e+00,\n",
       "       -6.03086715e-01, -2.80652217e-01, -8.08881375e-01, -4.66851003e-01,\n",
       "        1.72936068e-01,  1.82950756e+00, -5.70504173e-01, -1.59098672e-01,\n",
       "        7.03151985e-01, -6.29420533e-01, -4.06221904e-01, -7.53701700e-01,\n",
       "       -6.65520072e-01,  1.15973621e-01,  1.09044316e+00,  8.94312472e-01,\n",
       "        1.15556939e+00,  1.96533438e-01,  6.46276156e-01, -5.88693724e-01,\n",
       "        1.42530819e-02,  8.00499890e-01, -4.86127404e-01,  8.96133465e-01,\n",
       "        1.26998717e+00, -1.57136107e-01, -6.13063470e-01, -9.32339939e-01,\n",
       "       -1.36998227e-01, -1.49691033e+00,  2.46930174e-01, -1.64965358e-01,\n",
       "        3.74913169e-01,  3.49400950e-01, -1.20174369e+00,  9.62013098e-01,\n",
       "        1.65367157e-01, -1.16648444e-01, -1.16670918e+00,  3.97538444e-02,\n",
       "        6.05227033e-01, -9.31869680e-02,  1.35288679e+00, -2.51930355e-01,\n",
       "        4.56958878e-01,  8.81043640e-01, -5.99288160e-03, -3.97048292e-01,\n",
       "       -2.82953059e-01, -3.07499074e-01, -1.11587349e+00,  1.82520684e+00,\n",
       "        2.17561000e-01, -9.23572395e-01, -5.87537450e-01, -5.50270541e-01,\n",
       "        3.87835674e-01,  8.10898920e-03,  4.00873054e-01,  3.44544175e-01,\n",
       "       -1.36540655e-01,  6.64702555e-02,  1.67694302e-02,  2.22988631e+00,\n",
       "        2.83409624e-01, -3.74232302e-01, -3.98800527e-01,  9.36484914e-01,\n",
       "        1.87093331e-01,  1.07423387e+00, -1.13311531e+00,  3.82501531e-02,\n",
       "       -9.05792448e-02, -4.39630823e-02, -1.09495802e-01, -3.04330397e-02,\n",
       "        2.14475060e-01, -3.66211466e-02, -3.15310495e-01, -3.52431653e+00,\n",
       "        1.37837428e-01, -1.37725823e+00,  1.77151761e-01,  7.05131768e-01,\n",
       "       -6.67679703e-01,  3.34139412e-01, -5.34277819e-01,  1.66365043e-01,\n",
       "        2.40490571e-02,  1.06180422e+00,  1.07581175e-01,  3.44744830e-01,\n",
       "        1.36631399e-01,  3.96285134e-01, -1.30274429e+00, -6.61643996e-01,\n",
       "       -6.81588242e-01,  3.26044542e-01,  2.59750102e-01, -6.31191555e-01,\n",
       "       -3.60701249e-01, -7.58537686e-01, -2.22091580e-01,  6.97138431e-01,\n",
       "        1.16112386e+00, -6.23900970e-02,  2.15245487e-01,  3.57335008e-01,\n",
       "       -2.33325064e+00,  2.01169920e-01, -3.30304598e-01, -2.03781693e-01,\n",
       "       -4.55257070e-01,  4.87869068e-01, -6.06361593e-01, -2.80171668e-01,\n",
       "       -2.80182168e-01, -5.02537493e-01, -1.31622853e-01,  6.16406485e-01,\n",
       "        5.78773003e-01,  4.14614919e-01, -1.92494405e-01,  1.09541676e+00,\n",
       "        1.14050776e+00,  4.34097843e-01,  5.72256559e-01,  4.42194473e-01,\n",
       "       -1.12456967e+00, -6.40158564e-01,  3.89182789e-02, -2.94523889e+00,\n",
       "       -2.95396135e-01,  3.85164849e-01, -3.64014920e-01, -5.95806577e-01,\n",
       "       -3.88209732e-01, -1.59600519e+00, -9.99795125e-01,  3.05152033e-01])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y - estimated_conditional_outcomes.T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b606047",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
