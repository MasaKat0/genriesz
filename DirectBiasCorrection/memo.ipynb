{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5851bea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import optimize\n",
    "\n",
    "\n",
    "class RKHS_RieszLearner:\n",
    "    def __init__(self, loss, separate_or_share, link_name):\n",
    "        if loss == \"LS\":\n",
    "            self.loss_func = self.ls_loss\n",
    "        if loss == \"KL\":\n",
    "            self.loss_func = self.kl_loss\n",
    "        if loss == \"TL\":\n",
    "            self.loss_func = self.tailored_loss\n",
    "        \n",
    "        self.separate_or_share = separate_or_share\n",
    "        self.link_name = link_name\n",
    "\n",
    "    def _model_construction(self, param, X1, X0, treatment):\n",
    "        if self.link_name == \"Linear\":\n",
    "            if (self.separate_or_share == \"Separate\"):\n",
    "                param1 = param[:int(len(param)/2)]\n",
    "                param0 = param[int(len(param)/2):]\n",
    "                fx1 = X1 @ param1\n",
    "                fx0 = X0 @ param0\n",
    "            else:\n",
    "                fx1 = X1 @ param\n",
    "                fx0 = X0 @ param\n",
    "            alpha1 = fx1\n",
    "            alpha0 = fx0\n",
    "            alpha = alpha0\n",
    "            alpha[treatment] = alpha1[treatment]\n",
    "            \n",
    "        if self.link_name == \"Logit\":\n",
    "            fx1 = X1 @ param\n",
    "            ex1 = 1/(1 + np.exp(-fx1))\n",
    "            fx0 = X0 @ param\n",
    "            ex0 = 1/(1 + np.exp(-fx0))\n",
    "            alpha = treatment / ex1 - (1 - treatment) / (1 - ex0)\n",
    "\n",
    "        return alpha\n",
    "        \n",
    "    def ls_loss(self, param, X1, X0, treatment, regularizer):\n",
    "        treatment0 = treatment*0\n",
    "        treatment1 = treatment0 + 1\n",
    "        alpha1 = self._model_construction(param, X1, X0, treatment1)\n",
    "        alpha0 = self._model_construction(param, X1, X0, treatment0)\n",
    "        loss = - 2*(alpha1 - alpha0) + treatment*alpha1**2 + (1 - treatment)*alpha0**2\n",
    "        loss = np.mean(loss) + regularizer * np.sum(param**2)\n",
    "        return loss\n",
    "    \n",
    "    def kl_loss(self, param, X1, X0, treatment, regularizer):\n",
    "        treatment0 = treatment*0\n",
    "        treatment1 = treatment0 + 1\n",
    "        alpha1 = self._model_construction(param, X1, X0, treatment1)\n",
    "        alpha0 = self._model_construction(param, X1, X0, treatment0)\n",
    "        loss = - np.log(alpha1) - np.log(-alpha0) + treatment*alpha1 - (1 - treatment)*alpha0\n",
    "        loss = np.mean(loss) + regularizer * np.sum(param**2)\n",
    "        return loss\n",
    "    \n",
    "    def tailored_loss(self, param, X1, X0, treatment, regularizer):\n",
    "        treatment0 = treatment*0\n",
    "        treatment1 = treatment0 + 1\n",
    "        alpha1 = self._model_construction(param, X1, X0, treatment1)\n",
    "        alpha0 = self._model_construction(param, X1, X0, treatment0)\n",
    "        loss = - (1 - treatment)*np.log(alpha1 - 1) - treatment*np.log(- alpha0 - 1) + treatment*alpha1 - (1 - treatment)*alpha0\n",
    "        loss = np.mean(loss) + regularizer * np.sum(param**2)\n",
    "        return loss\n",
    "        \n",
    "    def optimize(self, covariate, treatment, x_test):\n",
    "        result = self.minimize(covariate, treatment, 0.01)\n",
    "        self.x_test = x_test\n",
    "        \n",
    "    def obj_func_gen(self, X1, X0, treatment, regularizer):                \n",
    "        obj_func = lambda param: self.loss_func(param, X1, X0, treatment, regularizer)\n",
    "        return obj_func\n",
    "    \n",
    "    def fit(self, covariate, treatment, outcome, folds=2, num_basis=50):\n",
    "        self.treatment = treatment\n",
    "        self.X1_train, self.X0_train, self.X_test, self.lda_chosen = model.kernel_cv(covariate, treatment, covariate, folds=2, num_basis=50)\n",
    "        self.train(self.X1_train, self.X0_train, self.treatment, self.lda_chosen)\n",
    "            \n",
    "    def train(self, X1, X0, treatment, lda_chosen, folds=2, num_basis=50):\n",
    "        obj_func = self.obj_func_gen(X1, X0, treatment, lda_chosen)\n",
    "        \n",
    "        if (self.separate_or_share == \"Share\") & (self.link_name == \"Linear\"):\n",
    "            init_param = np.zeros(X1.shape[1]*2)\n",
    "        else:\n",
    "            init_param = np.zeros(X1.shape[1])\n",
    "\n",
    "        self.result = optimize.minimize(obj_func, init_param, method=\"BFGS\")\n",
    "        self.params = self.result.x\n",
    "                    \n",
    "    def predict(self):\n",
    "        if self.link_name == \"Linear\":\n",
    "            if (self.separate_or_share == \"Share\"):\n",
    "                param1 = self.params[:int(len(self.params)/2)]\n",
    "                param0 = self.params[int(len(self.params)/2):]\n",
    "                fx1 = self.X_test @ param1\n",
    "                fx0 = self.X_test @ param0\n",
    "                \n",
    "                fx = fx0\n",
    "                fx[self.treatment == 1] = fx1[self.treatment == 1]\n",
    "                \n",
    "                alpha = fx\n",
    "            else:\n",
    "                fx = self.X_test @ self.params\n",
    "                alpha = fx\n",
    "            \n",
    "        if self.link_name == \"Logit\":\n",
    "            fx = self.X_test @ self.params\n",
    "            ex = 1/(1 + np.exp(-fx))\n",
    "            alpha = treatment / fx - (1 - treatment) / (1 - fx)\n",
    "\n",
    "\n",
    "        self.riesz = alpha\n",
    "                    \n",
    "        return self.riesz\n",
    "            \n",
    "\n",
    "    def dist(self, X, X1, X0, treatment=None, num_basis=False):\n",
    "        (d,n) = X.shape\n",
    "        \n",
    "        if num_basis is False:\n",
    "            num_basis = 1000\n",
    "\n",
    "        idx = np.random.permutation(n)[0:num_basis]\n",
    "        C = X[:, idx]\n",
    "\n",
    "        # calculate the squared distances\n",
    "        X1C_dist = CalcDistanceSquared(X1, C)\n",
    "        X0C_dist = CalcDistanceSquared(X0, C)\n",
    "        DC_dist = CalcDistanceSquared(treatment, C)\n",
    "        CC_dist = CalcDistanceSquared(C, C)\n",
    "        return X1C_dist, X0C_dist, DC_dist, CC_dist, n, num_basis\n",
    "\n",
    "\n",
    "    def kernel_cv(self, covariate_train, treatment, covariate_test, folds=5, num_basis=False, sigma_list=None, lda_list=None):\n",
    "        if self.separate_or_share == \"DX\":\n",
    "            treatment0 = treatment*0\n",
    "            treatment1 = treatment0 + 1\n",
    "            X_train1 = np.concatenate([np.array([treatment1]).T, covariate_train], axis=1)\n",
    "            X_train0 = np.concatenate([np.array([treatment0]).T, covariate_train], axis=1)\n",
    "            X_train = np.concatenate([np.array([treatment]).T, covariate_train], axis=1)\n",
    "        elif self.separate_or_share == \"OnlyX\":\n",
    "            X_train1 = covariate_train\n",
    "            X_train0 = covariate_train\n",
    "            X_train = covariate_train\n",
    "                    \n",
    "        if self.separate_or_share == \"DX\":\n",
    "            X_test = np.concatenate([np.array([treatment]).T, covariate_test], axis=1)\n",
    "        elif self.separate_or_share == \"OnlyX\":\n",
    "            X_test = covariate_test\n",
    "            \n",
    "        X_train, X_train1, X_train0, X_test = X_train.T, X_train1.T, X_train0.T, X_test.T\n",
    "        X1C_dist, X0C_dist, DC_dist, CC_dist, n, num_basis = self.dist(X_train, X_train1, X_train0, X_test, num_basis)\n",
    "        # setup the cross validation\n",
    "        cv_fold = np.arange(folds) # normal range behaves strange with == sign\n",
    "        cv_split0 = np.floor(np.arange(n)*folds/n)\n",
    "        cv_index = cv_split0[np.random.permutation(n)]\n",
    "        # set the sigma list and lambda list\n",
    "        if sigma_list==None:\n",
    "            sigma_list = np.array([0.01, 0.05, 0.1, 0.5, 1])\n",
    "        if lda_list==None:\n",
    "            lda_list = np.array([0.01, 0.05, 0.1, 0.5, 1])\n",
    "        score_cv = np.zeros((len(sigma_list), len(lda_list)))\n",
    "        \n",
    "        for sigma_idx, sigma in enumerate(sigma_list):\n",
    "            # pre-sum to speed up calculation\n",
    "            h1_cv = []\n",
    "            h0_cv = []\n",
    "            d_cv = []\n",
    "            for k in cv_fold:\n",
    "                h1_cv.append(np.exp(-X1C_dist[:, cv_index==k]/(2*sigma**2)))\n",
    "                h0_cv.append(np.exp(-X0C_dist[:, cv_index==k]/(2*sigma**2)))\n",
    "                d_cv.append(treatment[cv_index==k])\n",
    "\n",
    "            for k in range(folds):\n",
    "                # calculate the h vectors for training and test\n",
    "                count = 0\n",
    "                for j in range(folds):\n",
    "                    if j == k:\n",
    "                        h1te = h1_cv[j].T\n",
    "                        h0te = h0_cv[j].T\n",
    "                        dte = d_cv[j]\n",
    "                    else:\n",
    "                        if count == 0:\n",
    "                            h1tr = h1_cv[j].T\n",
    "                            h0tr = h0_cv[j].T\n",
    "                            dtr = d_cv[j]\n",
    "                            count += 1\n",
    "                        else:\n",
    "                            h1tr = np.append(h1tr, h1_cv[j].T, axis=0)\n",
    "                            h0tr = np.append(h0tr, h0_cv[j].T, axis=0)\n",
    "                            dtr = np.append(dtr, d_cv[j], axis=0)\n",
    "\n",
    "                one = np.ones((len(h1tr),1))\n",
    "                h1tr = np.concatenate([h1tr, one], axis=1)\n",
    "                h0tr = np.concatenate([h0tr, one], axis=1)\n",
    "                one = np.ones((len(h1te),1))\n",
    "                h1te = np.concatenate([h1te, one], axis=1)\n",
    "                h0te = np.concatenate([h0te, one], axis=1)\n",
    "                for lda_idx, lda in enumerate(lda_list):\n",
    "                    res_param = self.train(h1tr, h0tr, dtr, lda)\n",
    "                    # calculate the solution and cross-validation value\n",
    "                    obj_func = self.obj_func_gen(h1te, h0te, dte, 0)\n",
    "                    score = obj_func(self.params)       \n",
    "                    score_cv[sigma_idx, lda_idx] = score_cv[sigma_idx, lda_idx] + score\n",
    "\n",
    "\n",
    "        # get the minimum\n",
    "        (sigma_idx_chosen, lda_idx_chosen) = np.unravel_index(np.argmin(score_cv), score_cv.shape)\n",
    "        sigma_chosen = sigma_list[sigma_idx_chosen]\n",
    "        lda_chosen = lda_list[lda_idx_chosen]\n",
    "\n",
    "        x1_train = np.exp(-X1C_dist/(2*sigma_chosen**2)).T\n",
    "        x0_train = np.exp(-X0C_dist/(2*sigma_chosen**2)).T\n",
    "        x_test = np.exp(-DC_dist/(2*sigma_chosen**2)).T\n",
    "\n",
    "        one = np.ones((len(x1_train),1))\n",
    "        X1_train = np.concatenate([x1_train, one], axis=1)\n",
    "        X0_train = np.concatenate([x0_train, one], axis=1)\n",
    "        one = np.ones((len(x_test),1))\n",
    "        X_test = np.concatenate([x_test, one], axis=1)\n",
    "        \n",
    "        return X1_train, X0_train, X_test, lda_chosen\n",
    "\n",
    "\n",
    "\n",
    "def CalcDistanceSquared(X, C):\n",
    "    '''\n",
    "    Calculates the squared distance between X and C.\n",
    "    XC_dist2 = CalcDistSquared(X, C)\n",
    "    [XC_dist2]_{ij} = ||X[:, j] - C[:, i]||2\n",
    "    :param X: dxn: First set of vectors\n",
    "    :param C: d:nc Second set of vectors\n",
    "    :return: XC_dist2: The squared distance nc x n\n",
    "    '''\n",
    "    X**2\n",
    "    Xsum = np.sum(X**2, axis=0).T\n",
    "    Csum = np.sum(C**2, axis=0)\n",
    "    XC_dist = Xsum[np.newaxis, :] + Csum[:, np.newaxis] - 2*np.dot(C.T, X)\n",
    "    return XC_dist\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
