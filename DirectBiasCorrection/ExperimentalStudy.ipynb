{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32cd9a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.special import expit  # Sigmoid function for propensity score\n",
    "from kernel_regression import KernelRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99e11ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation parameters\n",
    "n = 3000 # Number of samples\n",
    "p = 3   # Number of covariates\n",
    "treatment_effect = 5.0  # True treatment effect\n",
    "\n",
    "# Generate covariates\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa011270",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_ATE = treatment_effect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62df913a",
   "metadata": {},
   "source": [
    "### Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aac5f325",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trial = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7fe2342",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsims = 1000\n",
    "np.random.seed(123)\n",
    "methods = ['dr', 'direct', 'ips']\n",
    "srr = {'dr' : True, 'direct' : False, 'ips' : True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 906,
   "id": "81ff8605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import optimize\n",
    "\n",
    "\n",
    "class RKHS_RieszLearner:\n",
    "    def __init__(self, method, model_name, link_name):\n",
    "        if method == \"LS\":\n",
    "            self.loss_func = self.ls_loss\n",
    "        if method == \"KL\":\n",
    "            self.loss_func = self.kl_loss\n",
    "        if method == \"EB\":\n",
    "            self.loss_func = self.eb_loss\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.link_name = link_name\n",
    "\n",
    "    def _model_construction(self, param, X1, X0, treatment):\n",
    "        if self.link_name == \"Linear\":\n",
    "            if (self.model_name == \"Common\"):\n",
    "                param1 = param[:int(len(param)/2)]\n",
    "                param0 = param[int(len(param)/2):]\n",
    "                fx1 = X1 @ param1\n",
    "                fx0 = X0 @ param0\n",
    "            else:\n",
    "                fx1 = X1 @ param\n",
    "                fx0 = X0 @ param\n",
    "            alpha1 = fx1\n",
    "            alpha0 = fx0\n",
    "            alpha = alpha0\n",
    "            alpha[treatment] = alpha1[treatment]\n",
    "            \n",
    "        if self.link_name == \"Logistic\":\n",
    "            fx1 = X1 @ param\n",
    "            ex1 = 1/(1 + np.exp(-fx1))\n",
    "            fx0 = X0 @ param\n",
    "            ex0 = 1/(1 + np.exp(-fx0))\n",
    "            alpha = treatment / ex1 - (1 - treatment) / (1 - ex0)\n",
    "\n",
    "        return alpha\n",
    "        \n",
    "    def ls_loss(self, param, X1, X0, treatment, regularizer):\n",
    "        treatment0 = treatment*0\n",
    "        treatment1 = treatment0 + 1\n",
    "        alpha1 = self._model_construction(param, X1, X0, treatment1)\n",
    "        alpha0 = self._model_construction(param, X1, X0, treatment0)\n",
    "        loss = - 2*(alpha1 - alpha0) + treatment*alpha1**2 + (1 - treatment)*alpha0**2\n",
    "        loss = np.mean(loss) + regularizer * np.sum(param**2)\n",
    "        return loss\n",
    "    \n",
    "    def kl_loss(self, param, X1, X0, treatment, regularizer):\n",
    "        treatment0 = treatment*0\n",
    "        treatment1 = treatment0 + 1\n",
    "        alpha1 = self._model_construction(param, X1, X0, treatment1)\n",
    "        alpha0 = self._model_construction(param, X1, X0, treatment0)\n",
    "        loss = - np.log(alpha1) - np.log(-alpha0) + treatment*alpha1 - (1 - treatment)*alpha0\n",
    "        loss = np.mean(loss) + regularizer * np.sum(param**2)\n",
    "        return loss\n",
    "    \n",
    "    def eb_loss(self, param, X1, X0, treatment, regularizer):\n",
    "        treatment0 = treatment*0\n",
    "        treatment1 = treatment0 + 1\n",
    "        alpha1 = self._model_construction(param, X1, X0, treatment1)\n",
    "        alpha0 = self._model_construction(param, X1, X0, treatment0)\n",
    "        loss = - (1 - treatment)*np.log(alpha1 - 1) - treatment*np.log(- alpha0 - 1) + treatment*alpha1 - (1 - treatment)*alpha0\n",
    "        loss = np.mean(loss) + regularizer * np.sum(param**2)\n",
    "        print(loss)\n",
    "        return loss\n",
    "        \n",
    "    def optimize(self, covariate, treatment, x_test):\n",
    "        result = self.minimize(covariate, treatment, 0.01)\n",
    "        self.x_test = x_test\n",
    "        \n",
    "    def obj_func_gen(self, X1, X0, treatment, regularizer):                \n",
    "        obj_func = lambda param: self.loss_func(param, X1, X0, treatment, regularizer)\n",
    "        return obj_func\n",
    "    \n",
    "    def fit(self, X1, X0, treatment, regularizer):        \n",
    "        obj_func = self.obj_func_gen(X1, X0, treatment, regularizer)\n",
    "        \n",
    "        if (self.model_name == \"Common\") & (self.link_name == \"Linear\"):\n",
    "            init_param = np.zeros(X1.shape[1]*2)\n",
    "        else:\n",
    "            init_param = np.zeros(X1.shape[1])\n",
    "\n",
    "        self.result = optimize.minimize(obj_func, init_param, method=\"BFGS\")\n",
    "        self.params = self.result.x\n",
    "                \n",
    "        return self.params\n",
    "    \n",
    "    def test(self, x, b, t, quant=True, pi=False):\n",
    "        theta = 0\n",
    "        f = np.dot(x, b)\n",
    "        if quant is True:\n",
    "            temp = np.copy(f)\n",
    "            temp = np.sort(temp)\n",
    "            theta = temp[np.int(np.floor(len(x)*(1-pi)))]\n",
    "        pred = np.zeros(len(x))\n",
    "        pred[f > theta] = 1\n",
    "        acc = np.mean(pred == t)\n",
    "        return acc\n",
    "\n",
    "    def dist(self, X, X1, X0, treatment=None, num_basis=False):\n",
    "        (d,n) = X.shape\n",
    "        \n",
    "        if num_basis is False:\n",
    "            num_basis = 1000\n",
    "\n",
    "        idx = np.random.permutation(n)[0:num_basis]\n",
    "        C = X[:, idx]\n",
    "\n",
    "        # calculate the squared distances\n",
    "        X1C_dist = CalcDistanceSquared(X1, C)\n",
    "        X0C_dist = CalcDistanceSquared(X0, C)\n",
    "        DC_dist = CalcDistanceSquared(treatment, C)\n",
    "        CC_dist = CalcDistanceSquared(C, C)\n",
    "        return X1C_dist, X0C_dist, DC_dist, CC_dist, n, num_basis\n",
    "\n",
    "\n",
    "    def kernel_cv(self, covariate_train, treatment, covariate_test, folds=5, num_basis=False, sigma_list=None, lda_list=None):\n",
    "        if self.model_name == \"Separate\":\n",
    "            treatment0 = treatment*0\n",
    "            treatment1 = treatment0 + 1\n",
    "            X_train1 = np.concatenate([np.array([treatment1]).T, covariate_train], axis=1)\n",
    "            X_train0 = np.concatenate([np.array([treatment0]).T, covariate_train], axis=1)\n",
    "            X_train = np.concatenate([np.array([treatment]).T, covariate_train], axis=1)\n",
    "        elif self.model_name == \"Common\":\n",
    "            X_train1 = covariate_train\n",
    "            X_train0 = covariate_train\n",
    "            X_train = covariate_train\n",
    "                    \n",
    "        if self.model_name == \"Separate\":\n",
    "            X_test = np.concatenate([np.array([treatment]).T, covariate_test], axis=1)\n",
    "        elif self.model_name == \"Common\":\n",
    "            X_test = covariate_test\n",
    "            \n",
    "        X_train, X_train1, X_train0, X_test = X_train.T, X_train1.T, X_train0.T, X_test.T\n",
    "        X1C_dist, X0C_dist, DC_dist, CC_dist, n, num_basis = self.dist(X_train, X_train1, X_train0, X_test, num_basis)\n",
    "        # setup the cross validation\n",
    "        cv_fold = np.arange(folds) # normal range behaves strange with == sign\n",
    "        cv_split0 = np.floor(np.arange(n)*folds/n)\n",
    "        cv_index = cv_split0[np.random.permutation(n)]\n",
    "        # set the sigma list and lambda list\n",
    "        if sigma_list==None:\n",
    "            sigma_list = np.array([0.01, 0.05, 0.1, 0.5, 1])\n",
    "        if lda_list==None:\n",
    "            lda_list = np.array([0.01, 0.05, 0.1, 0.5, 1])\n",
    "        score_cv = np.zeros((len(sigma_list), len(lda_list)))\n",
    "        \n",
    "        for sigma_idx, sigma in enumerate(sigma_list):\n",
    "            # pre-sum to speed up calculation\n",
    "            h1_cv = []\n",
    "            h0_cv = []\n",
    "            d_cv = []\n",
    "            for k in cv_fold:\n",
    "                h1_cv.append(np.exp(-X1C_dist[:, cv_index==k]/(2*sigma**2)))\n",
    "                h0_cv.append(np.exp(-X0C_dist[:, cv_index==k]/(2*sigma**2)))\n",
    "                d_cv.append(treatment[cv_index==k])\n",
    "\n",
    "            for k in range(folds):\n",
    "                #print(h0_cv[0])\n",
    "                # calculate the h vectors for training and test\n",
    "                count = 0\n",
    "                for j in range(folds):\n",
    "                    if j == k:\n",
    "                        h1te = h1_cv[j].T\n",
    "                        h0te = h0_cv[j].T\n",
    "                        dte = d_cv[j]\n",
    "                    else:\n",
    "                        if count == 0:\n",
    "                            h1tr = h1_cv[j].T\n",
    "                            h0tr = h0_cv[j].T\n",
    "                            dtr = d_cv[j]\n",
    "                            count += 1\n",
    "                        else:\n",
    "                            h1tr = np.append(h1tr, h1_cv[j].T, axis=0)\n",
    "                            h0tr = np.append(h0tr, h0_cv[j].T, axis=0)\n",
    "                            dtr = np.append(dtr, d_cv[j], axis=0)\n",
    "\n",
    "                one = np.ones((len(h1tr),1))\n",
    "                h1tr = np.concatenate([h1tr, one], axis=1)\n",
    "                h0tr = np.concatenate([h0tr, one], axis=1)\n",
    "                one = np.ones((len(h1te),1))\n",
    "                h1te = np.concatenate([h1te, one], axis=1)\n",
    "                h0te = np.concatenate([h0te, one], axis=1)\n",
    "                for lda_idx, lda in enumerate(lda_list):\n",
    "                    res_param = self.fit(h1tr, h0tr, dtr, lda)\n",
    "                    # calculate the solution and cross-validation value\n",
    "                    obj_func = self.obj_func_gen(h1te, h0te, dte, 0)\n",
    "                    score = obj_func(res_param)       \n",
    "                    score_cv[sigma_idx, lda_idx] = score_cv[sigma_idx, lda_idx] + score\n",
    "\n",
    "\n",
    "        # get the minimum\n",
    "        (sigma_idx_chosen, lda_idx_chosen) = np.unravel_index(np.argmin(score_cv), score_cv.shape)\n",
    "        sigma_chosen = sigma_list[sigma_idx_chosen]\n",
    "        lda_chosen = lda_list[lda_idx_chosen]\n",
    "\n",
    "        x1_train = np.exp(-X1C_dist/(2*sigma_chosen**2)).T\n",
    "        x0_train = np.exp(-X0C_dist/(2*sigma_chosen**2)).T\n",
    "        x_test = np.exp(-DC_dist/(2*sigma_chosen**2)).T\n",
    "\n",
    "        one = np.ones((len(x1_train),1))\n",
    "        X1_train = np.concatenate([x1_train, one], axis=1)\n",
    "        X0_train = np.concatenate([x0_train, one], axis=1)\n",
    "        one = np.ones((len(x_test),1))\n",
    "        X_test = np.concatenate([x_test, one], axis=1)\n",
    "\n",
    "        print(34)\n",
    "        print(sigma_idx_chosen, lda_idx_chosen)\n",
    "        print(score_cv)\n",
    "        return X1_train, X0_train, X_test, lda_chosen\n",
    "\n",
    "\n",
    "\n",
    "def CalcDistanceSquared(X, C):\n",
    "    '''\n",
    "    Calculates the squared distance between X and C.\n",
    "    XC_dist2 = CalcDistSquared(X, C)\n",
    "    [XC_dist2]_{ij} = ||X[:, j] - C[:, i]||2\n",
    "    :param X: dxn: First set of vectors\n",
    "    :param C: d:nc Second set of vectors\n",
    "    :return: XC_dist2: The squared distance nc x n\n",
    "    '''\n",
    "\n",
    "    Xsum = np.sum(X**2, axis=0).T\n",
    "    Csum = np.sum(C**2, axis=0)\n",
    "    XC_dist = Xsum[np.newaxis, :] + Csum[:, np.newaxis] - 2*np.dot(C.T, X)\n",
    "    return XC_dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 907,
   "id": "198e0a05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.18565118e-11, 6.79439056e-05, 3.22634022e-13, ...,\n",
       "        1.84059842e-09, 2.47619531e-06, 1.00000000e+00],\n",
       "       [2.75622293e-02, 3.22251834e-02, 6.15494899e-05, ...,\n",
       "        1.06047907e-05, 4.25503599e-04, 1.00000000e+00],\n",
       "       [1.22726048e-03, 1.61491224e-02, 8.83784599e-08, ...,\n",
       "        2.75158156e-06, 9.83835031e-06, 1.00000000e+00],\n",
       "       ...,\n",
       "       [3.22669988e-04, 6.14002595e-04, 1.15061884e-04, ...,\n",
       "        7.44212513e-08, 7.19893974e-04, 1.00000000e+00],\n",
       "       [1.25119513e-01, 2.13852673e-02, 7.40176963e-04, ...,\n",
       "        6.76752796e-04, 2.08688479e-04, 1.00000000e+00],\n",
       "       [6.53007955e-01, 3.73671254e-03, 2.38549688e-03, ...,\n",
       "        1.46523109e-05, 4.72847434e-05, 1.00000000e+00]], shape=(6000, 51))"
      ]
     },
     "execution_count": 907,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 925,
   "id": "b8a16a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RKHS_RieszLearner(\"LS\", \"Common\", \"Logistic\")\n",
    "model = RKHS_RieszLearner(\"LS\", \"Separate\", \"Linear\")\n",
    "model = RKHS_RieszLearner(\"KL\", \"Common\", \"Logistic\")\n",
    "#model = RKHS_RieszLearner(\"EB\", \"Common\", \"Logistic\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 926,
   "id": "b52842a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "4 0\n",
      "[[1.10824566 1.11012325 1.11587363 1.15998682 1.18387747]\n",
      " [1.10820052 1.11011403 1.11586944 1.15998624 1.1838772 ]\n",
      " [1.10802471 1.11008038 1.11585443 1.1599847  1.18387656]\n",
      " [0.90062739 1.03097409 1.07358876 1.15180056 1.17944551]\n",
      " [0.6347628  0.84556489 0.9424132  1.0989529  1.14040871]]\n"
     ]
    }
   ],
   "source": [
    "X1_train, X0_train, X_test, lda_chosen = model.kernel_cv(covariate, treatment, covariate, folds=2, num_basis=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 927,
   "id": "9f586372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02373279, 0.3231449 , 0.01873902, ..., 0.00223222, 0.0300584 ,\n",
       "        1.        ],\n",
       "       [0.38445838, 0.09808698, 0.69215863, ..., 0.14695711, 0.39044814,\n",
       "        1.        ],\n",
       "       [0.07263832, 0.28381672, 0.18880815, ..., 0.0109396 , 0.64433763,\n",
       "        1.        ],\n",
       "       ...,\n",
       "       [0.97687847, 0.04870433, 0.66426542, ..., 0.57197381, 0.05521792,\n",
       "        1.        ],\n",
       "       [0.08937845, 0.43614616, 0.28891683, ..., 0.03396159, 1.        ,\n",
       "        1.        ],\n",
       "       [0.21443161, 0.09499593, 0.55045058, ..., 0.10753226, 0.52794689,\n",
       "        1.        ]], shape=(3000, 51))"
      ]
     },
     "execution_count": 927,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 928,
   "id": "1114fe0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02373279, 0.3231449 , 0.01873902, ..., 0.00223222, 0.0300584 ,\n",
       "        1.        ],\n",
       "       [0.38445838, 0.09808698, 0.69215863, ..., 0.14695711, 0.39044814,\n",
       "        1.        ],\n",
       "       [0.07263832, 0.28381672, 0.18880815, ..., 0.0109396 , 0.64433763,\n",
       "        1.        ],\n",
       "       ...,\n",
       "       [0.97687847, 0.04870433, 0.66426542, ..., 0.57197381, 0.05521792,\n",
       "        1.        ],\n",
       "       [0.08937845, 0.43614616, 0.28891683, ..., 0.03396159, 1.        ,\n",
       "        1.        ],\n",
       "       [0.21443161, 0.09499593, 0.55045058, ..., 0.10753226, 0.52794689,\n",
       "        1.        ]], shape=(3000, 51))"
      ]
     },
     "execution_count": 928,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X0_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 929,
   "id": "ef62d7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "param = model.fit(X1_train, X0_train, treatment, lda_chosen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 930,
   "id": "fd84f55d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.22290776, -0.26819854,  0.18263007, -0.0572777 ,  0.19897182,\n",
       "       -0.21363251, -0.00721066,  0.45429154, -0.06635056,  0.00962513,\n",
       "       -0.47264793,  0.29921256,  1.07548308, -0.16059467, -0.14323398,\n",
       "        0.07830309,  0.06265214,  0.25293698,  0.418337  , -0.2275189 ,\n",
       "       -0.01130893, -0.09943221, -0.12464728, -0.00139151,  0.04561794,\n",
       "        0.07747579,  0.56854383,  0.173371  , -0.38057247, -0.13678693,\n",
       "        0.1789083 , -0.55442239,  0.67900037,  0.28122544, -0.13518248,\n",
       "       -0.10261297,  0.18990896,  0.07894066,  0.24863382,  0.31490512,\n",
       "        0.27941754, -0.31034535, -0.14270491, -0.71864442,  0.3095028 ,\n",
       "       -0.10728542,  0.32880536,  0.36380871, -0.21887687, -0.34264033,\n",
       "       -1.28300225])"
      ]
     },
     "execution_count": 930,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 931,
   "id": "8522fb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = X_test @ param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 932,
   "id": "fd03a49d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 51)"
      ]
     },
     "execution_count": 932,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 933,
   "id": "071a82bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = 1/(1 + np.exp(-a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 934,
   "id": "d4e63aab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.18425408, 0.16558271, 0.12904567, ..., 0.4526736 , 0.18269181,\n",
       "       0.13731756], shape=(3000,))"
      ]
     },
     "execution_count": 934,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 935,
   "id": "04bbaf2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.48778727, -1.61726279, -1.90942314, ..., -0.18987399,\n",
       "       -1.49821564, -1.83775043], shape=(3000,))"
      ]
     },
     "execution_count": 935,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 936,
   "id": "a38e57a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0345672 , 0.16820832, 0.02368561, ..., 0.47667238, 0.16818766,\n",
       "       0.12640262], shape=(3000,))"
      ]
     },
     "execution_count": 936,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "propensity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 937,
   "id": "0d967581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(7.903922609236567)"
      ]
     },
     "execution_count": 937,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(treatment / e * outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 938,
   "id": "00a0f487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(3.18809604334634)"
      ]
     },
     "execution_count": 938,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((1 - treatment) / (1 - e) * outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 939,
   "id": "1c2bdf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = treatment / e - (1 - treatment) / (1 - e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 923,
   "id": "7cde8410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(4.5535276031829435)"
      ]
     },
     "execution_count": 923,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(alpha * outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 940,
   "id": "d1d5114d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(4.715826565890227)"
      ]
     },
     "execution_count": 940,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(alpha * outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "2c971be3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treatment_effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "3887e37a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[409]\u001b[39m\u001b[32m, line 47\u001b[39m\n\u001b[32m     40\u001b[39m agmm.fit(X_train, y_train, Xval=X_test, yval=y_test,\n\u001b[32m     41\u001b[39m          earlystop_rounds=\u001b[32m2\u001b[39m, earlystop_delta=earlystop_delta,\n\u001b[32m     42\u001b[39m          learner_lr=\u001b[32m1e-4\u001b[39m, learner_l2=learner_l2, learner_l1=learner_l1,\n\u001b[32m     43\u001b[39m          n_epochs=\u001b[32m100\u001b[39m, bs=bs, target_reg=target_reg,\n\u001b[32m     44\u001b[39m          riesz_weight=riesz_weight, optimizer=\u001b[33m'\u001b[39m\u001b[33madam\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     45\u001b[39m          model_dir=\u001b[38;5;28mstr\u001b[39m(Path.home()), device=device)\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# Fine tune\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m \u001b[43magmm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myval\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m         \u001b[49m\u001b[43mearlystop_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mearlystop_rounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearlystop_delta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mearlystop_delta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m         \u001b[49m\u001b[43mlearner_lr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearner_lr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearner_l2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearner_l2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearner_l1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearner_l1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m         \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m600\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_reg\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget_reg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m         \u001b[49m\u001b[43mriesz_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mriesz_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43madam\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m         \u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhome\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m params = \u001b[38;5;28mtuple\u001b[39m(x * y_scaler.scale_[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m method \u001b[38;5;129;01min\u001b[39;00m methods\n\u001b[32m     55\u001b[39m                \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m agmm.predict_avg_moment(XT, y,  model=\u001b[33m'\u001b[39m\u001b[33mfinal\u001b[39m\u001b[33m'\u001b[39m, method = method, srr = srr[method])) + (true_ATE, )\n\u001b[32m     57\u001b[39m result_list2.append(params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dropbox/DirectDebiasedMachineLearning/DirectBiasCorrection/utils/riesznet.py:229\u001b[39m, in \u001b[36mRieszNet.fit\u001b[39m\u001b[34m(self, X, y, Xval, yval, earlystop_rounds, earlystop_delta, learner_l2, learner_l1, learner_lr, n_epochs, bs, target_reg, riesz_weight, optimizer, warm_start, logger, model_dir, device, verbose)\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    201\u001b[39m \u001b[33;03mParameters\u001b[39;00m\n\u001b[32m    202\u001b[39m \u001b[33;03m----------\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    222\u001b[39m \u001b[33;03mverbose : whether to print messages related to progress of training\u001b[39;00m\n\u001b[32m    223\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    225\u001b[39m X, y, Xval, yval = \u001b[38;5;28mself\u001b[39m._pretrain(X, y, Xval, yval, bs=bs, warm_start=warm_start,\n\u001b[32m    226\u001b[39m                          logger=logger, model_dir=model_dir,\n\u001b[32m    227\u001b[39m                          device=device, verbose=verbose)\n\u001b[32m--> \u001b[39m\u001b[32m229\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mXval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myval\u001b[49m\u001b[43m=\u001b[49m\u001b[43myval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m            \u001b[49m\u001b[43mearlystop_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mearlystop_rounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearlystop_delta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mearlystop_delta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlearner_l2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearner_l2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearner_l1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearner_l1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlearner_lr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearner_lr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtarget_reg\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget_reg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mriesz_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mriesz_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[43m            \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m logger \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    237\u001b[39m     \u001b[38;5;28mself\u001b[39m.writer.flush()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dropbox/DirectDebiasedMachineLearning/DirectBiasCorrection/utils/riesznet.py:151\u001b[39m, in \u001b[36mRieszNet._train\u001b[39m\u001b[34m(self, X, y, Xval, yval, earlystop_rounds, earlystop_delta, learner_l2, learner_l1, learner_lr, n_epochs, bs, target_reg, riesz_weight, optimizer)\u001b[39m\n\u001b[32m    148\u001b[39m D_loss = torch.mean((yb - output[:, [\u001b[32m0\u001b[39m]]) ** \u001b[32m2\u001b[39m)\n\u001b[32m    149\u001b[39m D_loss += riesz_weight * torch.mean(- \u001b[32m2\u001b[39m * \u001b[38;5;28mself\u001b[39m.moment_fn(\n\u001b[32m    150\u001b[39m     xb, riesz_fn, \u001b[38;5;28mself\u001b[39m.device) + output[:, [\u001b[32m1\u001b[39m]] ** \u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m D_loss += target_reg * \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43myb\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    152\u001b[39m D_loss += L1_reg_loss\n\u001b[32m    154\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizerD.zero_grad()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "result_list = []\n",
    "\n",
    "for tr in range(num_trial):\n",
    "    result_list_temp = []\n",
    "    \n",
    "    # Generate covariates\n",
    "    covariate = np.random.normal(0, 1, (n, p))\n",
    "\n",
    "    # Define a propensity score model\n",
    "    covariate_temp = np.concatenate([covariate, covariate**2, np.array([covariate[:, 0]*covariate[:, 1], \n",
    "                                                                        covariate[:, 1]*covariate[:, 2], \n",
    "                                                                        covariate[:, 0]*covariate[:, 2]]).T], axis=1)\n",
    "    propensity_coef = np.random.normal(0, 0.5, covariate_temp.shape[1])\n",
    "    propensity_scores = expit(covariate_temp @ propensity_coef)  # Calculate propensity scores\n",
    "\n",
    "    # Generate treatment assignment based on propensity scores\n",
    "    treatment = np.random.binomial(1, propensity_scores)\n",
    "\n",
    "    # Generate outcome with treatment effect\n",
    "    beta1 = np.random.normal(0, 1, p)\n",
    "    gamma1 = np.random.normal(0, 1, p)\n",
    "    fx1 = covariate @ beta1\n",
    "    gx1 = covariate**2 @ gamma1\n",
    "    outcome = (fx1)**2 + 1/(1 + np.exp(-gx1)) + 1.1 + treatment_effect * treatment + np.random.normal(0, 1, n)\n",
    "    \n",
    "    \n",
    "    covariate_treatment = X[T == 1]\n",
    "    X_control = X[T == 0]\n",
    "    \n",
    "    y_scaler = StandardScaler(with_mean=True).fit(np.array([Y]).T)\n",
    "    y = y_scaler.transform(np.array([Y]).T)\n",
    "    XT = np.c_[T, X]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(XT, y, test_size = 0.2)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    learner = Learner(X_train.shape[1], n_hidden, drop_prob, 0, interaction_only=True)\n",
    "    agmm = RieszNet(learner, moment_fn)\n",
    "    # Fast training\n",
    "    agmm.fit(X_train, y_train, Xval=X_test, yval=y_test,\n",
    "             earlystop_rounds=2, earlystop_delta=earlystop_delta,\n",
    "             learner_lr=1e-4, learner_l2=learner_l2, learner_l1=learner_l1,\n",
    "             n_epochs=100, bs=bs, target_reg=target_reg,\n",
    "             riesz_weight=riesz_weight, optimizer='adam',\n",
    "             model_dir=str(Path.home()), device=device)\n",
    "    # Fine tune\n",
    "    agmm.fit(X_train, y_train, Xval=X_test, yval=y_test,\n",
    "             earlystop_rounds=earlystop_rounds, earlystop_delta=earlystop_delta,\n",
    "             learner_lr=learner_lr, learner_l2=learner_l2, learner_l1=learner_l1,\n",
    "             n_epochs=600, bs=bs, target_reg=target_reg,\n",
    "             riesz_weight=riesz_weight, optimizer='adam', warm_start=True,\n",
    "             model_dir=str(Path.home()), device=device)\n",
    "    \n",
    "    params = tuple(x * y_scaler.scale_[0] for method in methods\n",
    "                   for x in agmm.predict_avg_moment(XT, y,  model='final', method = method, srr = srr[method])) + (true_ATE, )\n",
    "                        \n",
    "    result_list2.append(params)\n",
    "    \n",
    "    Y = y.T[0]\n",
    "    \n",
    "    Y_treatment = Y[T == 1]\n",
    "    Y_control = Y[T == 0]\n",
    "    \n",
    "    XT = np.c_[T, X]\n",
    "    \n",
    "    outcome_model = MLPRegressor(random_state=1, max_iter=600)\n",
    "    outcome_model.fit(XT, Y)\n",
    "\n",
    "    T_treatment = T*0 + 1\n",
    "    T_control = T*0\n",
    "    XT_treatment = np.c_[T_treatment, X]\n",
    "    XT_control = np.c_[T_control, X]\n",
    "\n",
    "    est_treatment_outcome = outcome_model.predict(XT_treatment)\n",
    "    est_control_outcome = outcome_model.predict(XT_control)\n",
    "    \n",
    "    for method in [\"DBCLS\", \"DBCUKL\", \"CBPS\", \"Logit\"]:\n",
    "        prop_model = NeuralNetBiasCorrection(input_dim=p, lbd = 0., loss=method)\n",
    "        prop_model.fit(X, T)\n",
    "        est_prop_score = prop_model.predict_proba(X)[:, 1]\n",
    "        est_prop_score_dbc = est_prop_score\n",
    "\n",
    "        #treatment_outcome_model = KernelRegression(kernel=\"rbf\", gamma=np.logspace(-2, 2, 10))\n",
    "        #control_outcome_model = KernelRegression(kernel=\"rbf\", gamma=np.logspace(-2, 2, 10))\n",
    "\n",
    "        IPW_est = np.mean(T*Y / est_prop_score - (1 - T)*Y / (1 - est_prop_score))\n",
    "\n",
    "        # Evaluate performance\n",
    "        IPW_bias = IPW_est - true_ATE\n",
    "\n",
    "        result_list_temp.append(IPW_est)\n",
    "\n",
    "        DM_est = np.mean(est_treatment_outcome - est_control_outcome)\n",
    "\n",
    "        # Evaluate performance\n",
    "        DM_bias = DM_est - true_ATE\n",
    "        \n",
    "        result_list_temp.append(DM_est)\n",
    "\n",
    "        DR_est = np.mean(T*(Y - est_treatment_outcome) / est_prop_score - (1 - T)*(Y - est_control_outcome)  / (1 - est_prop_score) + est_treatment_outcome - est_control_outcome)\n",
    "\n",
    "        # Evaluate performance\n",
    "        DR_bias = DR_est - true_ATE\n",
    "\n",
    "        result_list_temp.append(DR_est)\n",
    "    \n",
    "    ##### Linear models\n",
    "    \n",
    "    # Fit a linear model to estimate the treatment effect\n",
    "    model = LinearRegression()\n",
    "    model.fit(np.hstack([X, T.reshape(-1, 1)]), Y)\n",
    "    estimated_treatment_effect = model.coef_[-1]\n",
    "\n",
    "    # Evaluate performance\n",
    "    true_ATE = treatment_effect\n",
    "    bias = estimated_treatment_effect - true_ATE\n",
    "    mse = mean_squared_error(Y, model.predict(np.hstack([X, T.reshape(-1, 1)])))\n",
    "\n",
    "    result_list_temp.append(estimated_treatment_effect)\n",
    "    \n",
    "    #### CBPS\n",
    "    \n",
    "    # Enable automatic conversion of Pandas DataFrame to R DataFrame\n",
    "    pandas2ri.activate()\n",
    "\n",
    "    # Simulate data in Python\n",
    "\n",
    "    # Create a pandas DataFrame\n",
    "    column_names = [f'X{i+1}' for i in range(p)]\n",
    "    df = pd.DataFrame(X, columns=column_names)\n",
    "    df['T'] = T\n",
    "    df['Y'] = Y\n",
    "\n",
    "\n",
    "    # Convert pandas DataFrame to R DataFrame\n",
    "    r_df = pandas2ri.py2rpy(df)\n",
    "\n",
    "    ro.r.assign(\"p\", p)\n",
    "\n",
    "    # Load the CBPS package in R and fit the model for ATE estimation\n",
    "    ro.r('''\n",
    "        library(CBPS)\n",
    "        estimate_cbps_ate <- function(df) {\n",
    "            formula_str <- paste(\"T ~\", paste(names(df)[1:{p}], collapse=\" + \"))\n",
    "\n",
    "            # CBPSの適用 (ATEの推定、ATT=0)\n",
    "            model <- CBPS(as.formula(formula_str), data = df, ATT = 0, method = \"exact\")\n",
    "\n",
    "            # 推定された傾向スコアの取得\n",
    "            df$propensity_score <- fitted(model)\n",
    "\n",
    "            # IPW (Inverse Probability Weighting) を適用\n",
    "            df$weight <- ifelse(df$T == 1, 1 / df$propensity_score, 1 / (1 - df$propensity_score))\n",
    "\n",
    "            # 重み付き回帰によるATEの推定\n",
    "            result <- lm(Y ~ T, data = df, weights = df$weight)\n",
    "\n",
    "            return(df$propensity_score)\n",
    "        }\n",
    "    ''')\n",
    "\n",
    "    # R関数を呼び出してATEと傾向スコアを取得\n",
    "    est_prop_score = ro.r['estimate_cbps_ate'](r_df)\n",
    "    \n",
    "    est_prop_score_cbps = est_prop_score\n",
    "    \n",
    "    #print(er)\n",
    "\n",
    "    IPW_est = np.mean(T*Y / est_prop_score - (1 - T)*Y / (1 - est_prop_score))\n",
    "\n",
    "    # Evaluate performance\n",
    "    IPW_bias = IPW_est - true_ATE\n",
    "    \n",
    "    result_list_temp.append(IPW_est)\n",
    "    \n",
    "    DM_est = np.mean(est_treatment_outcome - est_control_outcome)\n",
    "\n",
    "    # Evaluate performance\n",
    "    DM_bias = DM_est - true_ATE\n",
    "    \n",
    "    result_list_temp.append(DM_est)\n",
    "\n",
    "    DR_est = np.mean(T*(Y - est_treatment_outcome) / est_prop_score - (1 - T)*(Y - est_control_outcome)  / (1 - est_prop_score) + est_treatment_outcome - est_control_outcome)\n",
    "\n",
    "    # Evaluate performance\n",
    "    DR_bias = DR_est - true_ATE\n",
    "    \n",
    "    result_list_temp.append(DR_est)\n",
    "    \n",
    "    result_list_temp = np.array(result_list_temp)*y_scaler.scale_[0]\n",
    "    \n",
    "    result_list.append(result_list_temp)\n",
    "    \n",
    "    res = tuple(np.array(x) for x in zip(*result_list2))\n",
    "    truth = res[-1:]\n",
    "    res_dict = {}\n",
    "\n",
    "    res_list_temp = []\n",
    "    for it, method in enumerate(methods):\n",
    "        point, lb, ub = res[it * 3: (it + 1)*3]\n",
    "        res_list_temp.append(point)\n",
    "        \n",
    "    result_list_final = np.concatenate([np.array(result_list), np.array(res_list_temp).T], axis=1)\n",
    "    \n",
    "    print(np.round(np.sqrt(np.mean((result_list_final - true_ATE)**2, axis=0)), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cd2bba08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23.71877552,  7.80900173,  5.1638035 , ...,  6.21743204,\n",
       "        3.04346554,  7.66068435], shape=(3000,))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6416618d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e991ec93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8b086f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate covariates\n",
    "X = np.random.normal(0, 1, (n, p))\n",
    "\n",
    "# Define a propensity score model\n",
    "X_temp = np.concatenate([X, X**2, np.array([X[:, 0]*X[:, 1], X[:, 1]*X[:, 2], X[:, 0]*X[:, 2]]).T], axis=1)\n",
    "propensity_coef = np.random.normal(0, 0.5, X_temp.shape[1])\n",
    "propensity_scores = expit(X_temp @ propensity_coef)  # Calculate propensity scores\n",
    "\n",
    "# Generate treatment assignment based on propensity scores\n",
    "T = np.random.binomial(1, propensity_scores)\n",
    "\n",
    "# Generate outcome with treatment effect\n",
    "beta1 = np.random.normal(0, 1, p)\n",
    "gamma1 = np.random.normal(0, 1, p)\n",
    "fx1 = X @ beta1\n",
    "gx1 = X**2 @ gamma1\n",
    "Y1 = (fx1)**2 + 3/(1 + np.exp(-gx1)) + 1.1 + np.random.normal(0, 1, n)\n",
    "\n",
    "beta0 = np.random.normal(0, 1, p)\n",
    "gamma0 = np.random.normal(0, 1, p)\n",
    "\n",
    "fx0 = X @ beta0\n",
    "Y0 = (fx0 + 1)**2 + 1.1 + np.random.normal(0, 1, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "090e7bc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-1.9172335774802356)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adgustmenttreatment_effect / np.mean(Y1 - Y0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0594e015",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d977558",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
